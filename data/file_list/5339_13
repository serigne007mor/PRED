Return-Path: <p_bruneau@hotmail.com>
From: "Jean-Charles Lamirel" <lamirel@loria.fr>
To: polytech_liste-egc@univ-nantes.fr
Message-Id: <sympa.1660238940.18362.357@univ-nantes.fr>
Content-Type: text/plain; charset="UTF-8"
Content-Transfer-Encoding: QUOTED-PRINTABLE
MIME-Version: 1.0
Content-length: 5057
X-Mailer: Sympa 6.2.48
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] 5Th ICDM'22 IncrLearn Workshop

Last CFP:

5th ICDM Workshop on
Incremental classification and clustering, concept drift, novelty detection,
active learning in big/fast data context
(IncrLearn)
https://incrlearn.sciencesconf.org/

In conjunction with
22st IEEE International Conference on Data Mining (ICDM 2022)
Title: Incremental classification and clustering, concept drift, novelty
detection, active learning in big/fast data context

Description:

The development of dynamic information analysis methods, like incremental
classification/clustering, concept drift management novelty detection
techniques and active learning is becoming a central concern in a bunch of
applications whose main goal is to deal with information which is varying o=
ver
time or with information flows that can oversize memory storage or computat=
ion
capacity. These applications relate themselves to very various and highly
strategic domains, including web mining, social network analysis, adaptive
information retrieval, anomaly or intrusion detection, process control and
management recommender systems, technological and scientific survey, and ev=
en
genomic information analysis, in bioinformatics.
The term =E2=80=9Cincremental=E2=80=9D is often associated to the terms evo=
lutionary,
adaptive, interactive, on-line, or batch. Most of the learning methods were
initially defined in a non-incremental way. However, in each of these
families, were initiated incremental methods making it possible to consider
the temporal component of a data flow or to achieve learning on huge/fast
datasets in a tractable way. In a more general way incremental classificati=
on/
clustering algorithms and novelty detection approaches are subjected to the
following constraints:
1. Potential changes in the data description space must be considered;
2. Possibility to be applied without knowing as a preliminary all the data =
to
be analyzed;
3. Taking into account of a new data must be carried out without making
intensive use of the already considered data;
4. Result must but available after insertion of all new data.

The above-mentioned constraints clearly follow the VVV (Volume-Velocity and
Variety) rule and thus directly fit with big/fast data context.
This workshop aims to offer a meeting opportunity for academics and industr=
y-
related researchers, belonging to the various communities of Computational
Intelligence, Machine Learning, Experimental Design, Data Mining and Big/Fa=
st
Data Management to discuss new areas of incremental classification, concept
drift management and novelty detection and on their application to analysis=
 of
time varying information and huge dataset of various natures. Another
important aim
of the workshop is to bridge the gap between data acquisition or
experimentation and model building.

Through an exhaustive coverage of the incremental learning area workshop wi=
ll
provide fruitful exchanges between plenaries, contributors and workshop
attendees. The emerging big/fast data context will be taken into considerat=
ion
in the workshop.
The set of proposed incremental techniques includes, but is not limited to:

=E2=80=A2 Novelty detection algorithms and techniques
=E2=80=A2 Semi-supervised and active learning approaches
=E2=80=A2 Adaptive hierarchical, k-means or density-based methods
=E2=80=A2 Adaptive neural methods and associated Hebbian learning techniques
=E2=80=A2 Incremental deep learning
=E2=80=A2 Multiview diachronic approaches
=E2=80=A2 Probabilistic approaches
=E2=80=A2 Distributed approaches
=E2=80=A2 Graph partitioning methods and incremental clustering approaches =
based on
attributed graphs
=E2=80=A2 Incremental clustering approaches based on swarm intelligence and=
 genetic
algorithms
=E2=80=A2 Evolving classifier ensemble techniques
=E2=80=A2 Incremental classification methods and incremental classifier eva=
luation
=E2=80=A2 Dynamic feature selection techniques
=E2=80=A2 Clustering of time series
=E2=80=A2 Learning on data streams
=E2=80=A2 Visualization methods for evolving data analysis results

The list of application domain includes, but it is not limited to:

=E2=80=A2 Evolving textual information analysis
=E2=80=A2 Evolving social network analysis
=E2=80=A2 Dynamic process control and tracking
=E2=80=A2 Intrusion and anomaly detection
=E2=80=A2 Genomics and DNA micro-array data analysis
=E2=80=A2 Adaptive recommender and filtering systems
=E2=80=A2 Scientometrics, webometrics and technological survey
=E2=80=A2 Incremental learning in LPWAN and IoT context

Important dates:
=E2=80=A2 Paper submission: September 2, 2022
=E2=80=A2 Notification of acceptance: September 23, 2022
=E2=80=A2 Camera-ready: October 1, 2022
=E2=80=A2 ICDM 2022 Conference: November 30, 2022

Three renowned plenary speakers are already planed for the workshop !!

Submission Guidelines:
=E2=80=A2 Follow the regular submission guidelines of ICDM 2022
(https://www.wi-lab.com/cyberchair/2022/icdm22/scripts/submit.php?subarea=
=3DDM)
Paper will be triple blind reviewed. The accepted papers will appear in ICDM
workshops proceedings.
