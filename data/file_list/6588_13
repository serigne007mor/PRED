Return-Path: <adrian.popescu@cea.fr>
X-Original-To: polytech_liste-egc@sympa6.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa6.univ-nantes.prive
Received: from bouncesmtp2.univ-nantes.fr (BounceSMTP2.univ-nantes.prive [172.20.12.67])
	by sympa6.univ-nantes.prive (Postfix) with ESMTP id 4B3411813EFA
	for <polytech_liste-egc@sympa6.univ-nantes.prive>; Mon, 11 Nov 2013 10:38:25 +0100 (CET)
Received: from mx1.d101.univ-nantes.fr (MX1.univ-nantes.fr [193.52.101.135])
	by bouncesmtp2.univ-nantes.fr (Postfix) with ESMTP id 4978A676CA6
	for <polytech_liste-egc@sympa6.univ-nantes.prive>; Mon, 11 Nov 2013 10:38:25 +0100 (CET)
Received: from localhost (localhost [127.0.0.1])
	by mx1.d101.univ-nantes.fr (Postfix) with ESMTP id 3DE1C624062C
	for <liste-egc@polytech.univ-nantes.fr>; Mon, 11 Nov 2013 10:38:25 +0100 (CET)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: -104.9
X-Spam-Level: 
X-Spam-Status: No, score=-104.9 tagged_above=-1000 required=5
	tests=[CRM114_GOOD=-5, HTML_MESSAGE=0.001, RCVD_IN_DNSWL_HI=-100,
	SPF_PASS=-0.001, UN_PHISHING_PW=0.1] autolearn=disabled
X-CRM114-Status: GOOD ( 9.2809 )
X-CRM114-CacheID: 
Received: from mx1.d101.univ-nantes.fr ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id sB+Bt3RKpqW1 for <liste-egc@polytech.univ-nantes.fr>;
	Mon, 11 Nov 2013 10:38:20 +0100 (CET)
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from sainfoin-out.extra.cea.fr (sainfoin-out.extra.cea.fr [132.167.192.145])
	by mx1.d101.univ-nantes.fr (Postfix) with ESMTP id 6EFB141DC7C0
	for <liste-egc@polytech.univ-nantes.fr>; Mon, 11 Nov 2013 10:38:20 +0100 (CET)
Received: from pisaure.intra.cea.fr (pisaure.intra.cea.fr [132.166.88.21])
	by sainfoin.extra.cea.fr (8.14.2/8.14.2/CEAnet-Internet-out-2.3) with ESMTP id rAB9bxxi005844;
	Mon, 11 Nov 2013 10:37:59 +0100
Received: from pisaure.intra.cea.fr (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id C36F0201B92;
	Mon, 11 Nov 2013 10:38:09 +0100 (CET)
Received: from muguet2.intra.cea.fr (muguet2.intra.cea.fr [132.166.192.7])
	by pisaure.intra.cea.fr (Postfix) with ESMTP id B4F0D200F48;
	Mon, 11 Nov 2013 10:38:09 +0100 (CET)
Received: from EXCAH-B1.intra.cea.fr (excah-b1.intra.cea.fr [132.166.88.86])
	by muguet2.intra.cea.fr (8.13.8/8.13.8/CEAnet-Intranet-out-1.2) with ESMTP id rAB9bxHJ009160;
	Mon, 11 Nov 2013 10:37:59 +0100
Received: from EXDAG0-A3.intra.cea.fr ([fe80::25a1:c396:2106:4c5b]) by
 EXCAH-B1.intra.cea.fr ([fe80::b4bb:7f38:ea87:b760%11]) with mapi id
 14.02.0318.004; Mon, 11 Nov 2013 10:37:59 +0100
From: POPESCU Adrian 211643 <adrian.popescu@cea.fr>
To: "ln@cines.fr" <ln@cines.fr>,
        "liste-egc@polytech.univ-nantes.fr"
	<liste-egc@polytech.univ-nantes.fr>
Thread-Topic: MediaEval 2014 Call for Task Proposals
Thread-Index: Ac7ewbToTB4udPFmRq+oUtl1NNtSIw==
Date: Mon, 11 Nov 2013 09:37:58 +0000
Message-ID: <A3CBBA37AC11414DB0FF9BB6D0A5E4DC1F843332@EXDAG0-A3.intra.cea.fr>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [132.167.194.4]
x-tm-as-product-ver: SMEX-10.2.0.1135-7.000.1014-20234.007
x-tm-as-result: No--54.480200-0.000000-31
x-tm-as-user-approved-sender: Yes
x-tm-as-user-blocked-sender: No
Content-Type: multipart/alternative;
	boundary="_000_A3CBBA37AC11414DB0FF9BB6D0A5E4DC1F843332EXDAG0A3intrace_"
MIME-Version: 1.0
X-Validation-by: cyril.de-runz@univ-reims.fr
Subject: [liste-egc] MediaEval 2014 Call for Task Proposals


--_000_A3CBBA37AC11414DB0FF9BB6D0A5E4DC1F843332EXDAG0A3intrace_
Content-Type: text/plain; charset="Windows-1252"
Content-Transfer-Encoding: quoted-printable

Pourriez vous diffuser cet appel =E0 proposition de t=E2ches pour MediaEval=
 2014 ?

En vous remerciant,
Adrian Popescu

MediaEval 2014 Call for Task Proposals
MediaEval Multimedia Benchmark
http://www.multimediaeval.org/mediaeval2014
***Task proposal submission deadline: 21 December 2013***
__________________________________________________________

MediaEval is a benchmarking initiative dedicated to evaluating new algorith=
ms for multimedia access and retrieval. It emphasizes the 'multi' in multim=
edia and focuses on human and social aspects of multimedia tasks.

MediaEval is calling for proposals for tasks from researchers in academia a=
nd industry to run in the 2014 benchmarking season.

The proposal should contain the following elements:
- Name of the task,
- Short description of the use scenario underlying the task (Who would ulti=
mately use the technology developed to address this task?),
- Short description of the task (What is the problem that task participants=
 will be expected to solve?),
- Description of the data to be used, including a statement on how it is to=
 be licensed (Note that MediaEval encourages the use of Creative Commons da=
ta wherever possible.),
- Description of how the ground truth will be obtained,
- Statement of the evaluation metric and/or methodology,
- Brief statement of how the task is different from existing tasks in other=
 benchmarks and/or how it extends the previous year=92s MediaEval task (if =
applicable),
- Brief statement of why the task is a MediaEval task (Does the task involv=
e a strong social or human component?),
- Examples (2-3) of recommended reading (i.e., references of papers that yo=
u would expect participants to have read before attempting the task),
- Name and contact information for the members of the proposing team (Pleas=
e include a couple sentences about the composition/history of the team. New=
 collaborations are explicitly encouraged.),
- Summary (200-300 words) of the motivation, task, data and evaluation in a=
 form suitable for the survey (i.e., a condensed version including the most=
 important points from above),
- The survey asks if people are interested in the task, and also asks quest=
ions that gather people's input on certain task design decisions. Please ad=
d 4-7 questions that you would like potential participants to ask about the=
 task.

For the last two points, it is helpful to refer to last years survey to see=
 the format of the task description and the type of questions.
http://www.multimediaeval.org/docs/MediaEval2013_SurveyForm_FInal.pdf

There is no particular length specification for the proposal, some tasks wi=
ll require more explanation than others. However, proposals are easier to m=
anage if they are concise: in general, they should not exceed two pages.

Please email your proposal (as a .pdf) to Martha Larson m.a.larson@tudelft.=
nl and Gareth Jones gareth.jones@computing.dcu.ie by December 21, 2013.

__________________________________________________________

Task proposals are accepted on the basis of the existence of a community of=
 task supporters (i.e., researchers who are interested and would plan to pa=
rticipate in the task). Support is determined using a survey, which is circ=
ulated widely to the multimedia research community at the beginning of the =
year (January 2014). Task decisions are made mid-February. Tasks must also =
be viable given the design of the task and the resources available to the t=
ask organization team.

We encourage task proposers to join forces with colleagues from other insti=
tutions and other projects to create an organizing team large enough to bea=
r the burden of data set generation, results evaluation, and working notes =
paper review. Please contact Martha Larson m.a.larson@tudelft.nl if you hav=
e questions about task organization or if you are interested in being conne=
cted up with other people with similar interests and who could join togethe=
r to form a task organizer team.

MediaEval has been experiencing steady growth since it was founded in 2008 =
as a track called "VideoCLEF" within the CLEF benchmark campaign. In 2010, =
it became an independent benchmark and in 2012 it ran for the first time as=
 a fully "bottom-up benchmark", meaning that it is organized for the commun=
ity, by the community, independently of a "parent" project. The MediaEval b=
enchmarking season culminates with the MediaEval workshop. Participants com=
e together at the workshop to present and discuss their results, build coll=
aborations, and develop future task editions or entirely new tasks. Past wo=
rking notes proceedings of the workshop include:

MediaEval 2012: http://ceur-ws.org/Vol-807/
MediaEval 2013: http://ceur-ws.org/Vol-1043/

Example tasks that have run in past years are:
- Placing Task: Predict the geo-coordinates of user-contributed photos.
- Tagging Task: Automatically assign tags to user-generated videos.
- Spoken Web Search: Search FOR audio content WITHIN audio content USING an=
 audio content query.
- Search and Hyperlinking: Multi-modal search and automated hyperlinking of=
 user-generated and commercial video.
- Social Event Detection: Find multimedia items related to a particular eve=
nt within a social multimedia collection.
- Violent Scenes Detection Task: Automatically detect violence in movies.

We expect the MediaEval 2014 workshop to be held in October 2014 in Europe,=
 possibly returning to the venue of the MediaEval 2013 workshop in Barcelon=
a. For more information on the MediaEval Multimedia benchmark, please visit=
 http://www.multimediaeval.org/ or contact Martha Larson m.a.larson@tudelft=
.nl.

--_000_A3CBBA37AC11414DB0FF9BB6D0A5E4DC1F843332EXDAG0A3intrace_
Content-Type: text/html; charset="Windows-1252"
Content-Transfer-Encoding: quoted-printable

<html dir=3D"ltr">
<head>
<meta http-equiv=3D"Content-Type" content=3D"text/html; charset=3DWindows-1=
252">
<style id=3D"owaParaStyle" type=3D"text/css">P {margin-top:0;margin-bottom:=
0;}</style>
</head>
<body ocsi=3D"0" fpstyle=3D"1">
<div style=3D"direction: ltr;font-family: Tahoma;color: #000000;font-size: =
10pt;"><font size=3D"2"><span style=3D"font-size:10pt">Pourriez vous diffus=
er cet appel =E0 proposition de t=E2ches pour MediaEval 2014 ?<br>
<br>
En vous remerciant,<br>
Adrian Popescu<br>
<br>
MediaEval 2014 Call for Task Proposals<br>
MediaEval Multimedia Benchmark<br>
<a href=3D"http://www.multimediaeval.org/mediaeval2014" target=3D"_blank">h=
ttp://www.multimediaeval.org/mediaeval2014</a><br>
***Task proposal submission deadline: 21 December 2013***<br>
__________________________________________________________<br>
<br>
MediaEval is a benchmarking initiative dedicated to evaluating new algorith=
ms for multimedia access and retrieval. It emphasizes the 'multi' in multim=
edia and focuses on human and social aspects of multimedia tasks.<br>
<br>
MediaEval is calling for proposals for tasks from researchers in academia a=
nd industry to run in the 2014 benchmarking season.<br>
<br>
The proposal should contain the following elements:<br>
- Name of the task,<br>
- Short description of the use scenario underlying the task (Who would ulti=
mately use the technology developed to address this task?),<br>
- Short description of the task (What is the problem that task participants=
 will be expected to solve?),<br>
- Description of the data to be used, including a statement on how it is to=
 be licensed (Note that MediaEval encourages the use of Creative Commons da=
ta wherever possible.),<br>
- Description of how the ground truth will be obtained,<br>
- Statement of the evaluation metric and/or methodology,<br>
- Brief statement of how the task is different from existing tasks in other=
 benchmarks and/or how it extends the previous year=92s MediaEval task (if =
applicable),<br>
- Brief statement of why the task is a MediaEval task (Does the task involv=
e a strong social or human component?),<br>
- Examples (2-3) of recommended reading (i.e., references of papers that yo=
u would expect participants to have read before attempting the task),<br>
- Name and contact information for the members of the proposing team (Pleas=
e include a couple sentences about the composition/history of the team. New=
 collaborations are explicitly encouraged.),<br>
- Summary (200-300 words) of the motivation, task, data and evaluation in a=
 form suitable for the survey (i.e., a condensed version including the most=
 important points from above),<br>
- The survey asks if people are interested in the task, and also asks quest=
ions that gather people's input on certain task design decisions. Please ad=
d 4-7 questions that you would like potential participants to ask about the=
 task.<br>
<br>
For the last two points, it is helpful to refer to last years survey to see=
 the format of the task description and the type of questions.<br>
<a href=3D"http://www.multimediaeval.org/docs/MediaEval2013_SurveyForm_FIna=
l.pdf" target=3D"_blank">http://www.multimediaeval.org/docs/MediaEval2013_S=
urveyForm_FInal.pdf</a><br>
<br>
There is no particular length specification for the proposal, some tasks wi=
ll require more explanation than others. However, proposals are easier to m=
anage if they are concise: in general, they should not exceed two pages.<br>
<br>
Please email your proposal (as a .pdf) to Martha Larson m.a.larson@tudelft.=
nl and Gareth Jones gareth.jones@computing.dcu.ie by December 21, 2013.<br>
<br>
__________________________________________________________<br>
<br>
Task proposals are accepted on the basis of the existence of a community of=
 task supporters (i.e., researchers who are interested and would plan to pa=
rticipate in the task). Support is determined using a survey, which is circ=
ulated widely to the multimedia
 research community at the beginning of the year (January 2014). Task decis=
ions are made mid-February. Tasks must also be viable given the design of t=
he task and the resources available to the task organization team.<br>
<br>
We encourage task proposers to join forces with colleagues from other insti=
tutions and other projects to create an organizing team large enough to bea=
r the burden of data set generation, results evaluation, and working notes =
paper review. Please contact Martha
 Larson m.a.larson@tudelft.nl if you have questions about task organization=
 or if you are interested in being connected up with other people with simi=
lar interests and who could join together to form a task organizer team.<br>
<br>
MediaEval has been experiencing steady growth since it was founded in 2008 =
as a track called &quot;VideoCLEF&quot; within the CLEF benchmark campaign.=
 In 2010, it became an independent benchmark and in 2012 it ran for the fir=
st time as a fully &quot;bottom-up benchmark&quot;,
 meaning that it is organized for the community, by the community, independ=
ently of a &quot;parent&quot; project. The MediaEval benchmarking season cu=
lminates with the MediaEval workshop. Participants come together at the wor=
kshop to present and discuss their results,
 build collaborations, and develop future task editions or entirely new tas=
ks. Past working notes proceedings of the workshop include:<br>
<br>
MediaEval 2012: <a href=3D"http://ceur-ws.org/Vol-807/" target=3D"_blank">h=
ttp://ceur-ws.org/Vol-807/</a><br>
MediaEval 2013: <a href=3D"http://ceur-ws.org/Vol-1043/" target=3D"_blank">=
http://ceur-ws.org/Vol-1043/</a><br>
<br>
Example tasks that have run in past years are:<br>
- Placing Task: Predict the geo-coordinates of user-contributed photos.<br>
- Tagging Task: Automatically assign tags to user-generated videos.<br>
- Spoken Web Search: Search FOR audio content WITHIN audio content USING an=
 audio content query.<br>
- Search and Hyperlinking: Multi-modal search and automated hyperlinking of=
 user-generated and commercial video.<br>
- Social Event Detection: Find multimedia items related to a particular eve=
nt within a social multimedia collection.<br>
- Violent Scenes Detection Task: Automatically detect violence in movies.<b=
r>
<br>
We expect the MediaEval 2014 workshop to be held in October 2014 in Europe,=
 possibly returning to the venue of the MediaEval 2013 workshop in Barcelon=
a. For more information on the MediaEval Multimedia benchmark, please visit
<a href=3D"http://www.multimediaeval.org/" target=3D"_blank">http://www.mul=
timediaeval.org/</a> or contact Martha Larson m.a.larson@tudelft.nl.</span>=
</font></div>
</body>
</html>

--_000_A3CBBA37AC11414DB0FF9BB6D0A5E4DC1F843332EXDAG0A3intrace_--
