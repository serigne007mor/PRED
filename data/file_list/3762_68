Return-Path: <p_bruneau@hotmail.com>
X-Original-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Received: from bouncesmtp2.univ-nantes.fr (bouncesmtp2.u12.univ-nantes.prive [172.20.12.67])
	by sympa62.u12.univ-nantes.prive (Postfix) with ESMTP id 9642E14014F0
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Fri, 24 Mar 2023 12:10:26 +0100 (CET)
Received: from mx1.localdomain (MX1.univ-nantes.fr [193.52.101.135])
	by bouncesmtp2.univ-nantes.fr (Postfix) with ESMTP id 92C845FA2
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Fri, 24 Mar 2023 12:10:26 +0100 (CET)
Received: from localhost (localhost [127.0.0.1])
	by mx1.localdomain (Postfix) with ESMTP id 8C29A12067B
	for <liste-egc@polytech.univ-nantes.fr>; Fri, 24 Mar 2023 12:10:26 +0100 (CET)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: 1.409
X-Spam-Level: *
X-Spam-Status: No, score=1.409 tagged_above=-1000 required=5
	tests=[CRM114_UNSURE=0.1, HTML_MESSAGE=0.001, J_CHICKENPOX_21=0.6,
	J_CHICKENPOX_25=0.6, RCVD_IN_WSFF=0.01, SPF_HELO_PASS=-0.001,
	SPF_PASS=-0.001, UN_PHISHING_PW=0.1] autolearn=disabled
X-CRM114-Status: UNSURE ( 4.2487 )
X-CRM114-CacheID: 
Received: from mx1.localdomain ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id uZch4DNpeImC for <liste-egc@polytech.univ-nantes.fr>;
	Fri, 24 Mar 2023 12:10:24 +0100 (CET)
Received-SPF: Pass (mailfrom) identity=mailfrom; client-ip=2001:660:3302:283c::1e; helo=osiris.lip6.fr; envelope-from=marie-jeanne.lesot@lip6.fr; receiver=liste-egc@polytech.univ-nantes.fr 
Authentication-Results: dmarc.univ-nantes.fr; dmarc=none (p=none dis=none) header.from=lip6.fr
Authentication-Results: dmarc.univ-nantes.fr; spf=pass smtp.mailfrom=Marie-Jeanne.Lesot@lip6.fr
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from osiris.lip6.fr (osiris.lip6.fr [IPv6:2001:660:3302:283c::1e])
	by mx1.localdomain (Postfix) with ESMTPS id 3F0CE12000A
	for <liste-egc@polytech.univ-nantes.fr>; Fri, 24 Mar 2023 12:10:24 +0100 (CET)
Received: from poleia.lip6.fr (poleia.lip6.fr [132.227.201.8])
	by osiris.lip6.fr (8.16.1/8.15.2) with ESMTP id 32OBANbl004388
	for <liste-egc@polytech.univ-nantes.fr>; Fri, 24 Mar 2023 12:10:23 +0100 (CET)
Received: from [132.227.204.172] (averell.dapa.lip6.fr [132.227.204.172])
	by poleia.lip6.fr (Postfix) with ESMTPSA id 5DBC232AE40
	for <liste-egc@polytech.univ-nantes.fr>; Fri, 24 Mar 2023 12:10:23 +0100 (CET)
Content-Type: multipart/alternative;
 boundary="------------azpFT0CAUSMsT2pStErXfY0t"
Message-ID: <5b0a6cb3-7163-e520-14c5-d806ffbd6828@lip6.fr>
Date: Fri, 24 Mar 2023 12:10:23 +0100
MIME-Version: 1.0
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101
 Thunderbird/102.9.0
Content-Language: en-US
References: <dd9dc679-b570-86c6-2a64-6e6fec7e6617@lip6.fr>
To: liste-egc@polytech.univ-nantes.fr
From: Marie-Jeanne Lesot <Marie-Jeanne.Lesot@lip6.fr>
In-Reply-To: <dd9dc679-b570-86c6-2a64-6e6fec7e6617@lip6.fr>
X-Forwarded-Message-Id: <dd9dc679-b570-86c6-2a64-6e6fec7e6617@lip6.fr>
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.6.4 (osiris.lip6.fr [132.227.60.30]); Fri, 24 Mar 2023 12:10:23 +0100 (CET)
X-Scanned-By: MIMEDefang 3.3 on 132.227.60.30
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] =?UTF-8?Q?S=C3=A9minaire?= TRAIL vendredi 7 avril, 15h

This is a multi-part message in MIME format.
--------------azpFT0CAUSMsT2pStErXfY0t
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit

Bonjour
Dans le cadre des séminaires du laboratoire commun AXA/Sorbonne 
Université/LIP6 Trustworthy and Responsible AI Lab (TRAIL), nous avons 
le plaisir de vous convier à deux exposés le *vendredi 7 avril à 15h*


** *Human-Centered AI: How Can We Support End-Users to Interact with AI? **
***Katrien Verbert **(KU Leuven)
Despite the long history of work on explanations in the Machine 
Learning, AI and Recommender Systems literature, current efforts face 
unprecedented difficulties: contemporary models are more complex and 
less interpretable than ever. As such models are used in many day-to-day 
applications, justifying their decisions for non-expert users with 
little or no technical knowledge will only become more crucial. Although 
several explanation methods have been proposed, little work has been 
done to evaluate whether the proposed methods indeed enhance human 
interpretability. Many existing methods also require significant 
expertise and are static. Several researchers have voiced the need for 
interaction with explanations as a core requirement to support 
understanding. In this talk, I will present our work on explanation 
methods that are tailored to the needs of non-expert users in AI. In 
addition, I will present the results of several user studies that 
investigate how such explanations interact with different personal 
characteristics, such as expertise, need for cognition and visual 
working memory.

*** Visual Explanations for AI Decisions: Fostering Trust in AI through 
Transparency and Control*
**Jeroen Ooge** (KU Leuven)
Automated systems increasingly support decision-making with AI. While 
such automation often improves working efficiency, it also raises 
questions about the origin and validity of model outcomes. Explaining 
model outcomes is not trivial: AI models are black boxes to people 
unfamiliar with AI. A promising solution to realise explainable AI (XAI) 
is visualisation. Through interactive visualisations, people can better 
understand models’ behaviour and reasoning process, which helps them 
contextualise model outcomes. Important here is that different people 
and different contexts require different solutions. Thus, human-centred 
XAI methods are essential. In this talk, Jeroen will cover his XAI work 
on transparency and control, applied in healthcare and education. He 
will demonstrate some of the many visual interfaces he designed, and 
also present the user studies he conducted to study their impact on 
people’s behaviours, for example, their trust in AI decisions.


Les exposés auront lieu à la*salle de conférence de SCAI *(bâtiment 
esclangon, 1er étage, Jussieu) et seront diffusés en ligne.

Toutes les informations sont disponibles à l'adresse suivante
https://trail.lip6.fr/2023/03/02/trail-seminar-katrien-verbert-human-centered-ai-how-can-we-support-the-end-users-to-interact-with-ai/
Pour des raisons d'organisation, il est demandé de*s'incrire *sur cette 
même page.

Plus d'information : https://trail.lip6.fr/category/trail-seminars/

Cordialement,
Marie-Jeanne Lesot

--------------azpFT0CAUSMsT2pStErXfY0t
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: 8bit

<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  </head>
  <body>
    Bonjour<br>
    <div class="moz-forward-container"> Dans le cadre des séminaires du
      laboratoire commun AXA/Sorbonne Université/LIP6 Trustworthy and
      Responsible AI Lab (TRAIL), nous avons le plaisir de vous convier
      à deux exposés le <b>vendredi 7 avril à 15h</b><br>
      <br>
      <br>
      ** <b>Human-Centered AI: How Can We Support End-Users to Interact
        with AI? </b><b><br>
      </b><b><b>Katrien Verbert </b></b>(KU Leuven)<br>
      Despite the long history of work on explanations in the Machine
      Learning, AI and Recommender Systems literature, current efforts
      face unprecedented difficulties: contemporary models are more
      complex and less interpretable than ever. As such models are used
      in many day-to-day applications, justifying their decisions for
      non-expert users with little or no technical knowledge will only
      become more crucial. Although several explanation methods have
      been proposed, little work has been done to evaluate whether the
      proposed methods indeed enhance human interpretability. Many
      existing methods also require significant expertise and are
      static. Several researchers have voiced the need for interaction
      with explanations as a core requirement to support understanding.
      In this talk, I will present our work on explanation methods that
      are tailored to the needs of non-expert users in AI. In addition,
      I will present the results of several user studies that
      investigate how such explanations interact with different personal
      characteristics, such as expertise, need for cognition and visual
      working memory.<br>
      <br>
      <b>** Visual Explanations for AI Decisions: Fostering Trust in AI
        through Transparency and Control</b><br>
      <b><b>Jeroen Ooge</b></b> (KU Leuven) <br>
      Automated systems increasingly support decision-making with AI.
      While such automation often improves working efficiency, it also
      raises questions about the origin and validity of model outcomes.
      Explaining model outcomes is not trivial: AI models are black
      boxes to people unfamiliar with AI. A promising solution to
      realise explainable AI (XAI) is visualisation. Through interactive
      visualisations, people can better understand models’ behaviour and
      reasoning process, which helps them contextualise model outcomes.
      Important here is that different people and different contexts
      require different solutions. Thus, human-centred XAI methods are
      essential. In this talk, Jeroen will cover his XAI work on
      transparency and control, applied in healthcare and education. He
      will demonstrate some of the many visual interfaces he designed,
      and also present the user studies he conducted to study their
      impact on people’s behaviours, for example, their trust in AI
      decisions.<br>
      <br>
      <br>
      Les exposés auront lieu à la<b> salle de conférence de SCAI </b>(bâtiment
      esclangon, 1er étage, Jussieu) et seront diffusés en ligne.<br>
      <br>
      Toutes les informations sont disponibles à l'adresse suivante <br>
      <a class="moz-txt-link-freetext"
href="https://trail.lip6.fr/2023/03/02/trail-seminar-katrien-verbert-human-centered-ai-how-can-we-support-the-end-users-to-interact-with-ai/"
        moz-do-not-send="true">https://trail.lip6.fr/2023/03/02/trail-seminar-katrien-verbert-human-centered-ai-how-can-we-support-the-end-users-to-interact-with-ai/</a><br>
      Pour des raisons d'organisation, il est demandé de<b> s'incrire </b>sur
      cette même page. <br>
      <br>
      Plus d'information : <a class="moz-txt-link-freetext"
        href="https://trail.lip6.fr/category/trail-seminars/"
        moz-do-not-send="true">https://trail.lip6.fr/category/trail-seminars/</a><br>
      <br>
      Cordialement,<br>
      Marie-Jeanne Lesot<br>
      <br>
    </div>
  </body>
</html>

--------------azpFT0CAUSMsT2pStErXfY0t--
