Return-Path: <p_bruneau@hotmail.com>
X-Original-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Received: from bouncesmtp2.univ-nantes.fr (BounceSMTP2.univ-nantes.prive [172.20.12.67])
	by sympa62.u12.univ-nantes.prive (Postfix) with ESMTP id D79CA14014E0
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Fri,  8 Jan 2021 09:45:31 +0100 (CET)
Received: from mx2.d101.univ-nantes.fr (MX2.univ-nantes.fr [193.52.101.136])
	by bouncesmtp2.univ-nantes.fr (Postfix) with ESMTP id D628861F0E0
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Fri,  8 Jan 2021 09:45:31 +0100 (CET)
Received: from localhost (localhost [127.0.0.1])
	by mx2.d101.univ-nantes.fr (Postfix) with ESMTP id D1DC7B4B138
	for <liste-egc@polytech.univ-nantes.fr>; Fri,  8 Jan 2021 09:45:31 +0100 (CET)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: -4.978
X-Spam-Level:
X-Spam-Status: No, score=-4.978 tagged_above=-1000 required=5
	tests=[CRM114_GOOD=-5, DKIM_SIGNED=0.1, DKIM_VALID=-0.1,
	DKIM_VALID_AU=-0.1, DKIM_VALID_EF=-0.1, HTML_MESSAGE=0.001,
	NO_RDNS2=0.01, RCVD_IN_WSFF=0.01, SPF_HELO_NONE=0.001,
	SPF_PASS=-0.001, UN_PHISHING_COMPTE=0.1, UN_PHISHING_PW=0.1,
	URIBL_BLOCKED=0.001] autolearn=disabled
X-CRM114-Status: GOOD ( 5.9235 )
X-CRM114-CacheID: 
Authentication-Results: univ-nantes.fr (amavisd-new); dkim=pass (1024-bit key)
	header.d=univ-tours.fr
Received: from mx2.d101.univ-nantes.fr ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id uup4_ZKbmanS for <liste-egc@polytech.univ-nantes.fr>;
	Fri,  8 Jan 2021 09:45:29 +0100 (CET)
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from hermes.univ-tours.fr (hermes.univ-tours.fr [193.52.209.24])
	by mx2.d101.univ-nantes.fr (Postfix) with ESMTPS id 9E602A3DB48
	for <liste-egc@polytech.univ-nantes.fr>; Fri,  8 Jan 2021 09:45:29 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=univ-tours.fr; i=@univ-tours.fr; q=dns/txt; s=main;
  t=1610095529; x=1641631529;
  h=from:subject:to:cc:message-id:date:mime-version;
  bh=W2txPmJ2UebrTjCO3PzU02eoH1yegnZ0b9MX3QdCsDQ=;
  b=w+PuMOz/HPVg6E4N4Ows3U3E2xDWLYM3oGzSWkbfUiltDv1Q8IRjlSxG
   Dh2BcMVgtQGfWv5RXpi0LDz1vK5rFzW1wXc51AaaZ+LpbLZOqQh56nYuw
   NDFXkMtqV/e/chLNAPvgEMXzYHkWECxzYyJzZgJ+JGS2OoEfJtCWhC9UE
   s=;
IronPort-SDR: iMPfhXSVZB/HnilRrIfzgklxLb4u7icYVeYQHG5E/vmBJzs6YKvg2wwt+KbIqxSuMxt6rMENbn
 H9Fzg2YUOrIQ==
X-IPAS-Result: =?us-ascii?q?A2FnAwCoGvhfACoLfVgNVYEQhiGEAT6RDAgllCCIHQsBA?=
 =?us-ascii?q?QEPLwQBAYRKgXMmOBMCAwEBAQMCAwEBAQEGAQEBAQEBBQQBAQIMAYYIgn0pA?=
 =?us-ascii?q?YNxEBdFBQcFGT0HAmAMBgIBAYMilTSbEnaBMopSgTiFYYM7HYYzgTgMgjhsh?=
 =?us-ascii?q?1aCPiIEgls+I0EDCAaBJiK5IAeBWoEfkkaJCwUpolyOFIV8lSSMQoFtgXtNJ?=
 =?us-ascii?q?4M1UI5TFxSOE3M3AgYBCQEBAwkBjG8BAQ?=
IronPort-PHdr: =?us-ascii?q?9a23=3A6KNp8xZiyf6WQzB1V3XrBeX/LSx+4OfEezUN45?=
 =?us-ascii?q?9isYplN5qZps66Zh7h7PlgxGXEQZ/co6odzbaP7Oa6BTdLuMra+Fk5M7V0Hy?=
 =?us-ascii?q?cfjssXmwFySOWkMmbcaMDQUiohAc5ZX0Vk9XzoeWJcGcL5ekGA6ibqtW1aFR?=
 =?us-ascii?q?rwLxd6KfroEYDOkcu3y/qy+5rOaAlUmTaxe7x/IAi0oAnLqMUbjoVvJqksxh?=
 =?us-ascii?q?bGrHZDZvhby35vKV+PhRj3+92+/IRk8yReuvIh89BPXKDndKkmTrJWESorPX?=
 =?us-ascii?q?kt6MLkqRfMQw2P5mABUmoNiRpHHxLF7BDhUZjvtCbxq/dw1zObPc3ySrA0RC?=
 =?us-ascii?q?ii4qJ2QxLmlCsLKzg0+3zMh8dukKxUvg6upx1nw47Vfo6VMuZ+frjAdt8eXG?=
 =?us-ascii?q?ZNQ9pdWzBEDo66coABDfcOPfxAoof9ulUAsxiwCweiC+zg1jBFnWX50bEg3u?=
 =?us-ascii?q?k7DQ3L0g4tEtQTu3rUttX1M6ISXPi7wqbW0D7Nc+5W2TH86YPVbB0goeuMXb?=
 =?us-ascii?q?N0ccHMzkQgCwPEjk+RqYzhJT+ay/oCs3KG7+pmVOOikHMnqwdwojix38sslo?=
 =?us-ascii?q?7Jhp8PylDf7yl5zpw1KMS+RUVmbtGqDIFeuDuGN4tqXMwiWWdotT4nxrMJtp?=
 =?us-ascii?q?O2YicHxZo5yxPfdfCKfIqF7gzhWeuNLzl1h3xodryiihiy7ESt1PDxW9S73l?=
 =?us-ascii?q?hKrCdLndfBum4N2RHc7MWMV/hz/l+51DuM1A3f8PxILV40mKbBNpIt36Q8m5?=
 =?us-ascii?q?UNvUnFAyT4gl/5jLWMeUUh4uWo7uPnbaj4qZKELI90jx3+MrwpmsyiHeQ0Kg?=
 =?us-ascii?q?gOUHaf+eS7zLDs+VD1TbFUgvEsj6XUsZPXKd4HqqKjHgNZzIAs5w6+Dzegzt?=
 =?us-ascii?q?sYgWEKIVFGdR6dkoTlJVHDLOrlAfuimVihnylny+jDPrL7A5XNKnbDkK3mfb?=
 =?us-ascii?q?Z480NT1Aoyzcpe55JQEL0OPez8WlXstNzeDx40KBG0w+DmCNVjz48eRWWPAr?=
 =?us-ascii?q?KDMKPJq1OI6PgvL/GWZIAJoDb9N+Ql5/n2gHAig1AdZ7Wp3ZsJZ3C8APtmJk?=
 =?us-ascii?q?WYbWD3gtgfC2cKpBQxTPfxhV2fVz5TZmiyU7sg5jE8FYKmF5zDSpqjgLybxC?=
 =?us-ascii?q?27BIFZZnhaClCQFnflb4qEW/YIaCKWPMBhiSYIVaa/RI8nyRGuqRX6y7thLu?=
 =?us-ascii?q?rV4SAYqJTj1Nlr5+HKiREy8iF0D96G022XQWF0hHsCRyUq06BnvUx91lCD3L?=
 =?us-ascii?q?Bkg/xYEtxT4ulGUhwgOZHB0eN6FdfyWg3fcdeNVlmrWcimATcwTtIv2tMOf0?=
 =?us-ascii?q?F9G8i4jhDYxSaqDaUVnaSRBJMo6qLcw2TxJ8Flxnndyakhi10mTtJONWK8na?=
 =?us-ascii?q?N/+BXcB5TIk0qDjaqqe74c1jbX9Gif1WqOoF1YUAloXKXBXHAffVfWosrg6k?=
 =?us-ascii?q?zcQL+hFa8qMhNGyc6GKqpKdsPmgklGRPv5JdTeZWOxm3mpCRaO3LyMapbqdH?=
 =?us-ascii?q?sG3CnHBkgEiVNbwXHTMQklGirkrm3VJCRpEUPzbkjst/R4qW3+SEYy0wyRKU?=
 =?us-ascii?q?N7k/K/5xoRwP2VY/wfxLMN/ik7+BtuG1Po89+eMceGtUJKbb9dZZtp60Zd1G?=
 =?us-ascii?q?afuwFnI5qyB71kh0BbbwNqpVio2Q8hWdYIqtQjsH5/lFk6Eqmfyl4UMmrAhZ?=
 =?us-ascii?q?0=3D?=
X-IronPort-Anti-Spam-Filtered: true
X-IronPort-AV: E=Sophos;i="5.79,330,1602540000"; 
   d="scan'208,217";a="110447557"
X-Amp-Result: SKIPPED(no attachment in message)
Received: from mey07-1_migr-88-125-11-42.fbx.proxad.net (HELO [192.168.0.12]) ([88.125.11.42])
  by hermes3.univ-tours.fr with ESMTP/TLS/ECDHE-RSA-AES256-GCM-SHA384; 08 Jan 2021 09:45:28 +0100
From: De Runz Cyril <cyril.derunz@univ-tours.fr>
To: liste-egc@polytech.univ-nantes.fr, bull-ia@gdria.fr
Cc: devogele thomas <thomas.devogele@univ-tours.fr>,
 Venturini Gilles <gilles.venturini@univ-tours.fr>,
 Barthelemy Serres <barthelemy.serres@univ-tours.fr>
Message-ID: <61298afa-3b66-750f-1332-cb281204abab@univ-tours.fr>
Date: Fri, 8 Jan 2021 09:45:24 +0100
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:78.0) Gecko/20100101
 Thunderbird/78.6.0
MIME-Version: 1.0
Content-Type: multipart/alternative;
 boundary="------------C9E9FA01EE1C5EEC95A1B4D3"
Content-Language: fr
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] [Stage M2] Offre de stage de master 2
 =?UTF-8?Q?=C3=A0_l=27Universit=C3=A9?= de Tours (site de Blois) :
 =?UTF-8?Q?D=C3=A9tection?= d'objets et
 localisation dans les images sur smartphone

This is a multi-part message in MIME format.
--------------C9E9FA01EE1C5EEC95A1B4D3
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Bonjour à toutes et tous,

Nous proposons le sujet de stage ci-dessous qui peut ouvrir sur une 
thèse. Le stage sera à effectuer sur le site Blois avec possibilité de 
télétravail partiel.

Si vous souhaitez candidater, merci d’envoyer un CV, une lettre de 
motivation, vos notes des deux dernières années et des références de vos 
enseignants à cyril.derunz@univ-tours.fr

Cordialement,

Cyril de Runz

*********

*Détection d'objets et localisation dans les images sur smartphone **
*Financement pour 5 à 6 mois de stage à partir du 1er février pouvant 
être ajusté en fonction des contraintes du master.
Ce stage de Master 2, orienté Recherche, qui peut ouvrir sur une thèse, 
s'inscrit dans un projet de proposition d'un outil a destination des 
aveugles. Ce projet a pour but de développer des outils innovants basés 
sur des approches d'apprentissage profond pour l'aide aux déplacements 
des personnes malvoyantes.

Depuis quelques années, avec notamment l'avènement de l'apprentissage 
profond, l'IA fait des avancées importantes et son utilisation s'étend à 
de nombreux secteurs de l'activité humaine. Les jeunes handicapés sont 
soumis à un paradoxe : d'une part l'accès aux ressources numériques est 
une véritable révolution pour eux, mais d'autre part, ils sont souvent 
les laissés pour compte de ces progrès.  Ce stage  a pour mission de 
poser les jalons pour la production d'un assistant intelligent destiné à 
aider les handicapés visuels lors de leur déplacement.

Notre point de départ va concerner l'aide aux déplacements, avec trois 
volets liés entre eux. Une assistance intelligente doit permettre la 
reconnaissance d'objets dans l'environnement (objets du quotidien, 
objets personnels, objets mobiles, obstacles fixes, etc.), la 
cartographie de cet environnement pour adapter le déplacement à 
l'environnement direct, la définition d'interfaces homme-machine 
adéquates. Les objectifs de la thèse sur laquelle peut ouvrir ce stage 
portent sur trois thématiques de recherche en Informatique qui vont être 
liées entre elles par l'objectif que nous nous fixons : l'apprentissage 
machine à partir d'images, la géomatique, et l'interface homme/machine.  
Ce stage a pour objectif principal la question de l'apprentissage et de 
la reconnaissance des objets.

La détection d'objets à partir d'images/vidéos pour la reconnaissance de 
l'environnement proche a fortement bénéficié de l'avènement du Deep 
Learning allié aux évolutions récentes des capacités calculatoires 
[1,2]. Cependant, il reste de nombreux challenges pour l'instant non 
parfaitement couverts tels que la reconnaissance d'objets du quotidien, 
d'objets personnels, la détection d'obstacles fixes/mobiles, etc. Un 
aspect supplémentaire à la reconnaissance d'objets génériques concerne 
l'apprentissage en ligne et incrémental de la reconnaissance d'objet 
spécifiques choisis par la personne handicapée (comme ses objets 
personnels, l'entrée de son lycée, etc).

Une fois les objets détectés et reconnus, il faut pouvoir 
potentiellement les géolocalisés à l'aide des informations présentes 
dans les images et des données GPS du capteur exploité. Nous pourrions 
pour cela nous inspirer des travaux de [3,4] sur la localisation et 
l'identification des profondeurs.

L’objectif de ce stage est de proposer un premier outil permettant la 
reconnaissance d'objets à partir de photos en apprentissage profond et 
de leur localisation sur smartphone ANDROID. Après avoir fait un état de 
l'art sur les approches et outils existants, il s'agira donc de proposer 
un premier prototype.

Compétences attendues : Programmation Android, apprentissage (profond), 
notion de traitement d'images.

Le/La stagiaire sera encadré(e) par Cyril de Runz, Gilles Venturini, et 
Barthélémy Serres

Si vous souhaitez candidater, merci d’envoyer un CV, une lettre de 
motivation, vos notes des deux dernières années et des références de vos 
enseignants à cyril.derunz@univ-tours.fr

[1] Martineau, M., R. Raveaux, C. Chatelain, D. Conte, et G. Venturini 
(2018). Effectivetraining of convolutional neural networks for insect 
image recognition. In J. Blanc-Talon, D. Helbert, W. Philips, D. C. 
Popescu, et P. Scheunders (Eds.), Advanced Concepts for Intelligent 
Vision Systems, 19th International Conference, ACIVS 2018, Poitiers, 
France, September 24-27, 2018, Proceedings, Volume 11182 ofLecture 
Notesin Computer Science, pp. 426–437. Springer

[2] Martineau, M., R. Raveaux, D. Conte, et G. Venturini (2020). 
Learning error-correctinggraph matching with a multiclass neural 
network. Pattern Recognition Letters 134,68–76.

[3] Wang, L., M. Famouri, et A. Wong (2020).  Depthnet nano : A highly 
compactself-normalizing neural network for monocular depth estimation. 
arXiv preprintarXiv :2004.08008.

[4] Wang, L., A. Patnik, E. Wong, J. Wong, et A. Wong (2018). Oliv : An 
artificialintelligence-powered assistant for object localization for 
impaired vision .Journal ofComputational Vision and Imaging Systems 
4(1), 3–3



--------------C9E9FA01EE1C5EEC95A1B4D3
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: 8bit

<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  </head>
  <body>
    <p>Bonjour à toutes et tous,</p>
    <p>Nous proposons le sujet de stage ci-dessous qui peut ouvrir sur
      une thèse. Le stage sera à effectuer sur le site Blois avec
      possibilité de télétravail partiel.</p>
    <p>Si vous souhaitez candidater, merci d’envoyer un CV, une lettre
      de motivation, vos notes des deux dernières années et des
      références de vos enseignants à <a
        class="moz-txt-link-abbreviated"
        href="mailto:cyril.derunz@univ-tours.fr">cyril.derunz@univ-tours.fr</a>
    </p>
    <p> Cordialement,</p>
    <p>Cyril de Runz<br>
    </p>
    <p>*********</p>
    <p><b>Détection d'objets et localisation dans les images sur
        smartphone
      </b><b><br>
      </b>Financement pour 5 à 6 mois de stage à partir du 1er février
      pouvant être ajusté en fonction des contraintes du master.<br>
      Ce stage de Master 2, orienté Recherche, qui peut ouvrir sur une
      thèse, s'inscrit dans un projet de proposition d'un outil a
      destination des aveugles. Ce projet a pour but de développer des
      outils innovants basés sur des approches d'apprentissage profond
      pour l'aide aux déplacements des personnes malvoyantes.
      <br>
      <br>
      Depuis quelques années, avec notamment l'avènement de
      l'apprentissage profond, l'IA fait des avancées importantes et son
      utilisation s'étend à de nombreux secteurs de l'activité humaine.
      Les jeunes handicapés sont soumis à un paradoxe : d'une part
      l'accès aux ressources numériques est une véritable révolution
      pour eux, mais d'autre part, ils sont souvent les laissés pour
      compte de ces progrès.  Ce stage  a pour mission de poser les
      jalons pour la production d'un assistant intelligent destiné à
      aider les handicapés visuels lors de leur déplacement.
      <br>
      <br>
      Notre point de départ va concerner l'aide aux déplacements, avec
      trois volets liés entre eux. Une assistance intelligente doit
      permettre la reconnaissance d'objets dans l'environnement (objets
      du quotidien, objets personnels, objets mobiles, obstacles fixes,
      etc.), la cartographie de cet environnement pour adapter le
      déplacement à l'environnement direct, la définition d'interfaces
      homme-machine adéquates. Les objectifs de la thèse sur laquelle
      peut ouvrir ce stage portent sur trois thématiques de recherche en
      Informatique qui vont être liées entre elles par l'objectif que
      nous nous fixons : l'apprentissage machine à partir d'images, la
      géomatique, et l'interface homme/machine.  Ce stage a pour
      objectif principal la question de l'apprentissage et de la
      reconnaissance des objets.
      <br>
      <br>
      La détection d'objets à partir d'images/vidéos pour la
      reconnaissance de l'environnement proche a fortement bénéficié de
      l'avènement du Deep Learning allié aux évolutions récentes des
      capacités calculatoires [1,2]. Cependant, il reste de nombreux
      challenges pour l'instant non parfaitement couverts tels que la
      reconnaissance d'objets du quotidien, d'objets personnels, la
      détection d'obstacles fixes/mobiles, etc. Un aspect supplémentaire
      à la reconnaissance d'objets génériques concerne l'apprentissage
      en ligne et incrémental de la reconnaissance d'objet spécifiques
      choisis par la personne handicapée (comme ses objets personnels,
      l'entrée de son lycée, etc).
      <br>
      <br>
      Une fois les objets détectés et reconnus, il faut pouvoir
      potentiellement les géolocalisés à l'aide des informations
      présentes dans les images et des données GPS du capteur exploité.
      Nous pourrions pour cela nous inspirer des travaux de [3,4] sur la
      localisation et l'identification des profondeurs.
      <br>
      <br>
      L’objectif de ce stage est de proposer un premier outil permettant
      la reconnaissance d'objets à partir de photos en apprentissage
      profond et de leur localisation sur smartphone ANDROID. Après
      avoir fait un état de l'art sur les approches et outils existants,
      il s'agira donc de proposer un premier prototype.
      <br>
      <br>
      Compétences attendues : Programmation Android, apprentissage
      (profond), notion de traitement d'images.
      <br>
      <br>
      Le/La stagiaire sera encadré(e) par Cyril de Runz, Gilles
      Venturini, et Barthélémy Serres
      <br>
      <br>
      Si vous souhaitez candidater, merci d’envoyer un CV, une lettre de
      motivation, vos notes des deux dernières années et des références
      de vos enseignants à <a class="moz-txt-link-abbreviated"
        href="mailto:cyril.derunz@univ-tours.fr">cyril.derunz@univ-tours.fr</a>
      <br>
      <br>
      [1] Martineau, M., R. Raveaux, C. Chatelain, D. Conte, et G.
      Venturini (2018). Effectivetraining of convolutional neural
      networks for insect image recognition. In J. Blanc-Talon, D.
      Helbert, W. Philips, D. C. Popescu, et P. Scheunders (Eds.),
      Advanced Concepts for Intelligent Vision Systems, 19th
      International Conference, ACIVS 2018, Poitiers, France, September
      24-27, 2018, Proceedings, Volume 11182 ofLecture Notesin Computer
      Science, pp. 426–437. Springer
      <br>
      <br>
      [2] Martineau, M., R. Raveaux, D. Conte, et G. Venturini (2020).
      Learning error-correctinggraph matching with a multiclass neural
      network. Pattern Recognition Letters 134,68–76.
      <br>
      <br>
      [3] Wang, L., M. Famouri, et A. Wong (2020).  Depthnet nano : A
      highly compactself-normalizing neural network for monocular depth
      estimation. arXiv preprintarXiv :2004.08008.
      <br>
      <br>
      [4] Wang, L., A. Patnik, E. Wong, J. Wong, et A. Wong (2018). Oliv
      : An artificialintelligence-powered assistant for object
      localization for impaired vision .Journal ofComputational Vision
      and Imaging Systems 4(1), 3–3
      <br>
    </p>
    <p><br>
    </p>
  </body>
</html>

--------------C9E9FA01EE1C5EEC95A1B4D3--
