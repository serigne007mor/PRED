Return-Path: <p_bruneau@hotmail.com>
X-Original-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Received: from bouncesmtp2.univ-nantes.fr (BounceSMTP2.univ-nantes.prive [172.20.12.67])
	by sympa62.u12.univ-nantes.prive (Postfix) with ESMTP id A029614006F2
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Mon, 23 Nov 2020 10:45:08 +0100 (CET)
Received: from mx1.d101.univ-nantes.fr (MX1.univ-nantes.fr [193.52.101.135])
	by bouncesmtp2.univ-nantes.fr (Postfix) with ESMTP id 9EA4F61F0E4
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Mon, 23 Nov 2020 10:45:08 +0100 (CET)
Received: from localhost (localhost [127.0.0.1])
	by mx1.d101.univ-nantes.fr (Postfix) with ESMTP id 9A3B648081D3
	for <liste-egc@polytech.univ-nantes.fr>; Mon, 23 Nov 2020 10:45:08 +0100 (CET)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: -13.185
X-Spam-Level:
X-Spam-Status: No, score=-13.185 tagged_above=-1000 required=5
	tests=[BUY_DIPLOMA_3=1.5, CRM114_GOOD=-5, HTML_MESSAGE=0.001,
	LOTS_OF_MONEY=0.001, MR_NOT_ATTRIBUTED_IP=0.2, RCVD_IN_DNSWL_MED=-10,
	RCVD_IN_MSPIKE_H3=0.001, RCVD_IN_MSPIKE_WL=0.001, RCVD_IN_WSFF=0.01,
	SPF_HELO_NONE=0.001, SPF_PASS=-0.001, UN_PHISHING_PW=0.1,
	URIBL_BLOCKED=0.001] autolearn=disabled
X-CRM114-Status: GOOD ( 6.0306 )
X-CRM114-CacheID: 
Received: from mx1.d101.univ-nantes.fr ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id D7lsQ_P0On-T for <liste-egc@polytech.univ-nantes.fr>;
	Mon, 23 Nov 2020 10:45:06 +0100 (CET)
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from v-zimmta03.u-bordeaux.fr (v-zimmta03.u-bordeaux.fr [147.210.215.83])
	by mx1.d101.univ-nantes.fr (Postfix) with ESMTPS id 21CBA4800CCE
	for <liste-egc@polytech.univ-nantes.fr>; Mon, 23 Nov 2020 10:45:05 +0100 (CET)
Received: from v-zimmta03.u-bordeaux.fr (localhost [127.0.0.1])
	by v-zimmta03.u-bordeaux.fr (Postfix) with ESMTP id BE734180009C;
	Mon, 23 Nov 2020 10:45:05 +0100 (CET)
Received: from v-zimboxp12.srv.u-bordeaux.fr (v-zimboxp12.srv.u-bordeaux.fr [147.210.219.112])
	by v-zimmta03.u-bordeaux.fr (Postfix) with ESMTP id 23EE318119E4;
	Mon, 23 Nov 2020 10:45:05 +0100 (CET)
Date: Mon, 23 Nov 2020 10:45:04 +0100 (CET)
From: Georgeta Bordea <georgeta.bordea@u-bordeaux.fr>
To: corpora@uib.no, info-ic@inria.fr, liste-egc@polytech.univ-nantes.fr, 
	mt-list@eamt.org, moses-support@mit.edu
Cc: jean-luc.rouas@labri.fr
Message-ID: <1003863870.279034.1606124704941.JavaMail.zimbra@u-bordeaux.fr>
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="=_acc9c5b2-7fda-4a8e-91f6-a0c78c32fec9"
X-Originating-IP: [89.157.159.127]
X-Mailer: Zimbra 8.8.15_GA_3959 (ZimbraWebClient - GC86 (Win)/8.8.15_GA_3953)
Thread-Index: ZxMJJfeKJ9U9Gt8QzdA+B9nMNcY8Hw==
Thread-Topic: 2-year Postdoc position in Automatic Speech Recognition at University of Bordeaux
X-AV-Checked: ClamAV using ClamSMTP
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] 2-year Postdoc position in Automatic Speech Recognition
 at University of Bordeaux

--=_acc9c5b2-7fda-4a8e-91f6-a0c78c32fec9
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

The University of Bordeaux invites applications for a 2 year full-time post=
doctoral researcher in Automatic Speech Recognition. The position is part o=
f the FVLLMONTI project on efficient speech-to-speech translation for embed=
ded autonomous devices, funded by the European Community.

To apply, please send by email a single PDF file containing a full CV (incl=
uding publication list), cover letter (describing your personal qualificati=
ons, research interests and motivation for applying), evidence for software=
 development experience (active Github/Gitlab profile or similar), two of y=
our key publications, contact information of two referees and academic cert=
ificates (PhD, Diploma/Master, Bachelor certificates).

Details on the position are given below:

Job description: Post-doctoral position in Automatic Speech Recognition=20
Duration: 24 months
Starting date: as early as possible (from March 1st 2021)
Project: European FETPROACT project FVLLMONTI (starts January 2021)
Location: Bordeaux Computer Science Lab. (LaBRI CNRS UMR 5800), Bordeaux, F=
rance (Image and Sound team)
Salary: from 2 086,45EUR to 2 304,88EUR/month (estimated net salary after t=
axes, according to experience)

Contact: jean-luc.rouas@labri.fr

Short description:
The applicant will be in charge of developing state-of-the-art Automatic Sp=
eech Recognition systems for English and French as well as related Machine =
Translation systems using Deep Neural Networks. The objective is to provide=
 the exact specifications of the designed systems to the other partners of =
the project specialized in hardware. Adjustments will have to be made to ta=
ke into account the hardware constraints (i.e. memory and energy consumptio=
n impacting the number of parameters, computation time, ...) while keeping =
an eye on performance metrics (WER and BLEU scores). When a satisfactory tr=
ade-off is reached, more exploratory work is to be carried out on using emo=
tion/attitude/affect recognition on the speech samples to supply additional=
 information to the translation system.=20


Context of the project:
The aim of the FVLLMONTI project is to build a lightweight autonomous in-ea=
r device allowing speech-to-speech translation. Today, pocket-talk devices =
integrate IoT products requiring internet connectivity which, in general, i=
s proven to be energy inefficient. While machine translation (MT) and Natur=
al Language Processing (NLP) performances have greatly improved, an embedde=
d lightweight energy-efficient hardware remains elusive. Existing solutions=
 based on artificial neural networks (NNs) are computation-intensive and en=
ergy-hungry requiring server-based implementations, which also raises data =
protection and privacy concerns. Today, 2D electronic architectures suffer =
from "unscalable" interconnect and are thus still far from being able to co=
mpete with biological neural systems in terms of real-time information-proc=
essing capabilities with comparable energy consumption. Recent advances in =
materials science, device technology and synaptic architectures have the po=
tential to fill this gap with novel disruptive technologies that go beyond =
conventional CMOS technology. A promising solution comes from vertical nano=
wire field-effect transistors (VNWFETs) to unlock the full potential of tru=
ly unconventional 3D circuit density and performance.

Role:
The tasks assigned to the Computer Science lab are the design of the Automa=
tic Speech Recognition (for French and English) and the Machine Translation=
 (English to French and French to English) systems. Speech synthesis will n=
ot be explored in the project but an open-source implementation will be use=
d for demonstration purposes. Both ASR and MT tasks benefit from the use of=
 Transformer architectures over Convolutional (CNNs) or Recurrent (RNNs) ne=
ural network architectures. Thus, the role of the applicant will be to desi=
gn and implement state-of-the-art systems for ASR using Transformer network=
s (e.g. with the ESPNET toolkit) and to assist another post-doctorate for t=
he MT systems. Once the performances reached by these baseline systems are =
satisfactory, details on the network will be given to our hardware designer=
s partners (e.g. number of layers, value of the parameters, etc.). With the=
 feedback of these partners, adjustments will be made to the network consid=
ering the hardware constraints while trying not to degrade the performances=
 too much.

The second part of the project will focus on keeping up with the latest inn=
ovations and translating them into hardware specifications. For example, re=
cent research suggest that adding convolutional layers to the transformer a=
rchitecture (i.e. the "conformer" network) can help reduce the number of pa=
rameters of the model which is critical regarding the memory usage of the h=
ardware system.

Finally, more exploratory work on the detection of social affects (i.e. the=
 vocal expression of the intent of the speaker: 'politeness', 'irony', etc)=
 will be carried out. The additional information gathered using this detect=
ion will be added to the translation system for potential usage in the futu=
re speech synthesis system.=20

Required skills:
- PhD in Automatic Speech Recognition (preferred) or Machine Translation us=
ing deep neural networks
- Knowledge of most widely used toolboxes/frameworks (tensorflow, pytorch, =
espnet for example)
- Good programming skills (python)
- Good communication skills (frequent interactions with hardware specialist=
s)
- Interest in hardware design will be a plus

Selected references:
S. Karita et al., "A Comparative Study on Transformer vs RNN in Speech Appl=
ications," 2019 IEEE Automatic Speech Recognition and Understanding Worksho=
p (ASRU), SG, Singapore, 2019, pp. 449-456, doi: 10.1109/ASRU46091.2019.900=
3750.
Gulati, Anmol, et al. "Conformer: Convolution-augmented Transformer for Spe=
ech Recognition." arXiv preprint arXiv:2005.08100 (2020).
Rouas, Jean-Luc, et al. "Categorisation of spoken social affects in Japanes=
e: human vs. machine." ICPhS. 2019.=20

--=_acc9c5b2-7fda-4a8e-91f6-a0c78c32fec9
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<html><body><div style=3D"font-family: arial, helvetica, sans-serif; font-s=
ize: 12pt; color: #000000"><!--StartFragment--><pre style=3D"orphans: 2; te=
xt-align: start; text-indent: 0px; widows: 2; text-decoration-style: initia=
l; text-decoration-color: initial; overflow-wrap: break-word;" data-mce-sty=
le=3D"color: #000000; font-style: normal; font-variant-ligatures: normal; f=
ont-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans=
: 2; text-align: start; text-indent: 0px; text-transform: none; widows: 2; =
word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-style: i=
nitial; text-decoration-color: initial; overflow-wrap: break-word; white-sp=
ace: pre-wrap;"><span style=3D"white-space: pre-wrap;">The University of Bo=
rdeaux invites applications for a 2 year full-time postdoctoral researcher =
in Automatic Speech Recognition. The position is part of the FVLLMONTI proj=
ect on efficient speech-to-speech translation for embedded autonomous devic=
es, funded by the European Community.

To apply, please send by email a single PDF file containing a full CV (incl=
uding publication list), cover letter (describing your personal qualificati=
ons, research interests and motivation for applying), evidence for software=
 development experience (active Github/Gitlab profile or similar), two of y=
our key publications, contact information of two referees and academic cert=
ificates (PhD, Diploma/Master, Bachelor certificates).

Details on the position are given below:

Job description: Post-doctoral position in Automatic Speech Recognition=20
Duration: 24 months
Starting date: as early as possible (from March 1st 2021)
Project: European FETPROACT project FVLLMONTI (starts January 2021)
Location: Bordeaux Computer Science Lab. (LaBRI CNRS UMR 5800), Bordeaux, F=
rance (Image and Sound team)
Salary: from 2 086,45EUR to 2 304,88EUR/month (estimated net salary after t=
axes, according to experience)

Contact: jean-luc.rouas@labri.fr

Short description:
The applicant will be in charge of developing state-of-the-art Automatic Sp=
eech Recognition systems for English and French as well as related Machine =
Translation systems using Deep Neural Networks. The objective is to provide=
 the exact specifications of the designed systems to the other partners of =
the project specialized in hardware. Adjustments will have to be made to ta=
ke into account the hardware constraints (i.e. memory and energy consumptio=
n impacting the number of parameters, computation time, ...) while keeping =
an eye on performance metrics (WER and BLEU scores). When a satisfactory tr=
ade-off is reached, more exploratory work is to be carried out on using emo=
tion/attitude/affect recognition on the speech samples to supply additional=
 information to the translation system.=20


Context of the project:
The aim of the FVLLMONTI project is to build a lightweight autonomous in-ea=
r device allowing speech-to-speech translation. Today, pocket-talk devices =
integrate IoT products requiring internet connectivity which, in general, i=
s proven to be energy inefficient. While machine translation (MT) and Natur=
al Language Processing (NLP) performances have greatly improved, an embedde=
d lightweight energy-efficient hardware remains elusive. Existing solutions=
 based on artificial neural networks (NNs) are computation-intensive and en=
ergy-hungry requiring server-based implementations, which also raises data =
protection and privacy concerns. Today, 2D electronic architectures suffer =
from "unscalable" interconnect and are thus still far from being able to co=
mpete with biological neural systems in terms of real-time information-proc=
essing capabilities with comparable energy consumption. Recent advances in =
materials science, device technology and synaptic architectures have the po=
tential to fill this gap with novel disruptive technologies that go beyond =
conventional CMOS technology. A promising solution comes from vertical nano=
wire field-effect transistors (VNWFETs) to unlock the full potential of tru=
ly unconventional 3D circuit density and performance.

Role:
The tasks assigned to the Computer Science lab are the design of the Automa=
tic Speech Recognition (for French and English) and the Machine Translation=
 (English to French and French to English) systems. Speech synthesis will n=
ot be explored in the project but an open-source implementation will be use=
d for demonstration purposes. Both ASR and MT tasks benefit from the use of=
 Transformer architectures over Convolutional (CNNs) or Recurrent (RNNs) ne=
ural network architectures. Thus, the role of the applicant will be to desi=
gn and implement state-of-the-art systems for ASR using Transformer network=
s (e.g. with the ESPNET toolkit) and to assist another post-doctorate for t=
he MT systems. Once the performances reached by these baseline systems are =
satisfactory, details on the network will be given to our hardware designer=
s partners (e.g. number of layers, value of the parameters, etc.). With the=
 feedback of these partners, adjustments will be made to the network consid=
ering the hardware constraints while trying not to degrade the performances=
 too much.

The second part of the project will focus on keeping up with the latest inn=
ovations and translating them into hardware specifications. For example, re=
cent research suggest that adding convolutional layers to the transformer a=
rchitecture (i.e. the "conformer" network) can help reduce the number of pa=
rameters of the model which is critical regarding the memory usage of the h=
ardware system.

Finally, more exploratory work on the detection of social affects (i.e. the=
 vocal expression of the intent of the speaker: 'politeness', 'irony', etc)=
 will be carried out. The additional information gathered using this detect=
ion will be added to the translation system for potential usage in the futu=
re speech synthesis system.=20

Required skills:
- PhD in Automatic Speech Recognition (preferred) or Machine Translation us=
ing deep neural networks
- Knowledge of most widely used toolboxes/frameworks (tensorflow, pytorch, =
espnet for example)
- Good programming skills (python)
- Good communication skills (frequent interactions with hardware specialist=
s)
- Interest in hardware design will be a plus

Selected references:
S. Karita et al., "A Comparative Study on Transformer vs RNN in Speech Appl=
ications," 2019 IEEE Automatic Speech Recognition and Understanding Worksho=
p (ASRU), SG, Singapore, 2019, pp. 449-456, doi: 10.1109/ASRU46091.2019.900=
3750.
Gulati, Anmol, et al. "Conformer: Convolution-augmented Transformer for Spe=
ech Recognition." arXiv preprint arXiv:2005.08100 (2020).
Rouas, Jean-Luc, et al. "Categorisation of spoken social affects in Japanes=
e: human vs. machine." ICPhS. 2019.<br></span></pre><!--EndFragment--></div=
></body></html>
--=_acc9c5b2-7fda-4a8e-91f6-a0c78c32fec9--
