Return-Path: <p_bruneau@hotmail.com>
X-Original-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Received: from bouncesmtp2.univ-nantes.fr (bouncesmtp2.u12.univ-nantes.prive [172.20.12.67])
	by sympa62.u12.univ-nantes.prive (Postfix) with ESMTP id E8F6314014E0
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Sun, 27 Nov 2022 22:25:23 +0100 (CET)
Received: from mx2.localdomain (MX2.univ-nantes.fr [193.52.101.136])
	by bouncesmtp2.univ-nantes.fr (Postfix) with ESMTP id E16E55FA2
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Sun, 27 Nov 2022 22:25:23 +0100 (CET)
Received: from localhost (localhost [127.0.0.1])
	by mx2.localdomain (Postfix) with ESMTP id D6D811014C3
	for <liste-egc@polytech.univ-nantes.fr>; Sun, 27 Nov 2022 22:25:23 +0100 (CET)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: -3.999
X-Spam-Level:
X-Spam-Status: No, score=-3.999 tagged_above=-1000 required=5
	tests=[CRM114_GOOD=-5, DKIM_SIGNED=0.1, DKIM_VALID=-0.1,
	DKIM_VALID_AU=-0.1, DKIM_VALID_EF=-0.1, HTML_MESSAGE=0.001,
	MR_NOT_ATTRIBUTED_IP=0.2, RCVD_IN_WSFF=0.01,
	SARE_HEAD_8BIT_SPAM=0.888, SPF_HELO_NONE=0.001, UN_PHISHING_PW=0.1,
	URIBL_BLOCKED=0.001] autolearn=disabled
X-CRM114-Status: GOOD ( 9.0492 )
X-CRM114-CacheID: 
Authentication-Results: univ-nantes.fr (amavisd-new); dkim=pass (2048-bit key)
	header.d=univ-paris13.fr
Received: from mx2.localdomain ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id 7eJz27Q_PJRU for <liste-egc@polytech.univ-nantes.fr>;
	Sun, 27 Nov 2022 22:25:14 +0100 (CET)
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from smtp2.univ-paris13.fr (smtp2.univ-paris13.fr [81.194.43.176])
	by mx2.localdomain (Postfix) with ESMTPS id 08AD91014C2
	for <liste-egc@polytech.univ-nantes.fr>; Sun, 27 Nov 2022 22:25:14 +0100 (CET)
Received: from [192.168.1.138] (223.149.65.37.rev.sfr.net [37.65.149.223])
	(Authenticated sender: azzag)
	by smtp2.univ-paris13.fr (Postfix) with ESMTPSA id 9330A2521167;
	Sun, 27 Nov 2022 22:25:13 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=univ-paris13.fr;
	s=dkim; t=1669584313;
	h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
	 to:to:cc:mime-version:mime-version:content-type:content-type:
	 in-reply-to:in-reply-to:references:references;
	bh=2aPhb3HTSuLePHX+DL03kwH+bjhtZ7lLIhUmbdPgHTo=;
	b=LWzo/yRMxY7Jb/HHFNIBNr23B+c0oByKeddHCSYXXP0jkZwd9715hBjRXbg0rQh94fxk1T
	5egdUnjVFlRBiAnHcUcS4iWaB9msEEbbernlgwqDl3oCtUBZOwqpXpUaHPz/lc6kI667GL
	bSbIFRBZkGA7AhGQhcDEGtfWU2MniLI9ZR2Z2oijWF9lWoUUsgwMoHDbGCILbV7IPTW3wa
	unJ4txHapFwpfGQjJS32Sl9XZJJKEgVjBTaVq3phZZP6927+7RlnTIbiwgoqBFqaRslO7W
	/9YoZhFYK912o9yEAdAcIUKJyDED/h5Az8dRhSTQsbMtX0UfaVL8ghnLMEw+5w==
Content-Type: multipart/alternative;
 boundary="------------qO3Cs0p1sCXSG5944V2ZBqAp"
Message-ID: <d60ac14b-de94-f144-2b92-aa192f798687@univ-paris13.fr>
Date: Sun, 27 Nov 2022 22:25:17 +0100
MIME-Version: 1.0
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101
 Thunderbird/102.5.0
Content-Language: fr
From: Hanane AZZAG <azzag@univ-paris13.fr>
To: liste-egc@polytech.univ-nantes.fr, bull-i3@irit.fr
References: <c4c5bf2b-721d-7458-c5da-361cf083f152@univ-paris13.fr>
In-Reply-To: <c4c5bf2b-721d-7458-c5da-361cf083f152@univ-paris13.fr>
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] Atelier =?UTF-8?Q?M=C3=A9canismes_d=E2=80=99Attention?=
 et Apprentissage Automatique : =?UTF-8?Q?avanc=C3=A9es_r=C3=A9centes?= et
 perspectives @EGC'23 (extended deadline)

This is a multi-part message in MIME format.
--------------qO3Cs0p1sCXSG5944V2ZBqAp
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit

*********** extension de la date de soumission au 1er 
décembre*******************************


> ***********Atelier@EGC'23 : Mécanismes d'Attention et Apprentissage 
> Automatique : avancées récentes et 
> perspective**s***************************
>
> *https://m3a2023.sciencesconf.org/*_*
> *_
>
> _*Présentation de l'atelier*_
>
> Cet atelier a pour objectif de réunir les chercheurs intéressés par 
> les mécanismes d’attentions et leur apport dans le domaine de 
> l'apprentissage automatique. Notre ambition est de permettre aux 
> participants d'aborder tous les thèmes allant de la modélisation de 
> l’attention aux applications en passant par les architectures basées 
> sur de tels mécanismes. Les problématiques abordées lors de cet 
> atelier peuvent concerner les processus de modélisation, extraction 
> d’information, etc., ou les applications associées. L'atelier concerne 
> aussi bien _*l*__*es chercheurs du monde académique que ceux du 
> secteur industriel*_, et autant les notions conceptuelles que les 
> applications. L'atelier est ouvert en termes de propositions. Nous 
> souhaitons stimuler un échange et des discussions aussi bien du point 
> de vue théorique qu'expérimental :
>
>   *     Quels succès ont été récemment rencontrés, et quels échecs ?
>   *     Quel est l'apport ?
>   *     Comment l’évaluer ?
>   *     Quid de l'explicabilité ?
>
> Les présentations pourront concerner un travail abouti, des réflexions 
> sur la modélisation ou  un travail préliminaire, ainsi que la 
> réalisation de démonstrations.
> _*
> *__*Format des soumissions :*_
> Nous proposons deux types de soumissions :
>
>   *     Soumission classique : article 12 pages maximum
>   *     Soumission courte : article 2 pages maximum
>
> Les articles sont soumis suivant le format d'EGC. Les actes de 
> l'atelier seront de plus mis à disposition sur le Web.
>
> _*Dates importantes*_
>
>   *    Date limite de soumission des articles : 1er decembre :*27
>     novembre 2022.
>     *
>
>   *    Notification aux auteurs : 16 décembre.
>
> _*Lien de soumission :*_
> https://easychair.org/my/conference?conf=m3a2023
>
> _*Thèmes de l'atelier (liste non exhaustive) : *_
>
> Une liste de thèmes est donnée ci-dessous, reste ouverte et est non 
> limitative :
>
>   *     Réseaux de neurones et mécanismes d'attention (ex.
>     transformers,...)
>   *     Capacité des architectures basées sur l'attention à encoder et
>     extraire l'information pertinente
>   *     Combinaison des mécanismes d'attention et des approches
>     classiques d'apprentissage
>   *     IA frugale et complexité des mécanismes d'attention
>   *     Adaptabilité des mécanismes d'attention à l'apprentissage fédéré
>   *     Explicabilité des modèles basés sur l'attention (lien avec les
>     valeurs de Shapley...)
>   *     Les mécanismes d'attention dans les réseaux neuronaux profonds
>     et leur explication
>   *     Transférabilité des connaissances dans les modèles basés sur
>     l’attention
>   *     Données temporelles et mécanismes d'attention
>   *     Évaluation des valeurs générées par les mécanismes d'attention
>   *     Applications académiques et industrielles des modèles basés
>     sur l'attention
>   *     Visualisation optimale des attentions
>
> _*Organisateurs *_
>
>   *     Mohamed-Djallel DILMI - LIPN, USPN-UPXIII
>   *     Florent FOREST - IMOS, EPFL
>   *     Hanene AZZAG - LIPN, USPN-UPXIII
>   *     Mustpha LEBBAH - David Lab, Université Paris Saclay - Campus UVSQ
>
>
--
LIPN UMR 7030
Université Sorbonne Paris Nord
99, avenue J.-B. Clement
93430 Villetaneuse

--------------qO3Cs0p1sCXSG5944V2ZBqAp
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: 8bit

<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>
  <body>
    <p><font size="4">*********** extension de la date de soumission au
        1er décembre*******************************</font><br>
    </p>
    <div class="moz-cite-prefix"><br>
    </div>
    <blockquote type="cite"
      cite="mid:c4c5bf2b-721d-7458-c5da-361cf083f152@univ-paris13.fr">
      <meta http-equiv="content-type" content="text/html; charset=UTF-8">
      <b>********</b><b>Atelier@EGC'23 : Mécanismes d'Attention et
        Apprentissage Automatique : avancées récentes et perspective</b><b>s</b><b>************************</b>
      <p><b><a class="moz-txt-link-freetext"
            href="https://m3a2023.sciencesconf.org/"
            moz-do-not-send="true">https://m3a2023.sciencesconf.org/</a></b><u><b><br>
          </b></u></p>
      <p><u><b>Présentation de l'atelier</b></u></p>
      Cet atelier a pour objectif de réunir les chercheurs intéressés
      par les mécanismes d’attentions et leur apport dans le domaine de
      l'apprentissage automatique. Notre ambition est de permettre aux
      participants d'aborder tous les thèmes allant de la modélisation
      de l’attention aux applications en passant par les architectures
      basées sur de tels mécanismes. Les problématiques abordées lors de
      cet atelier peuvent concerner les processus de modélisation,
      extraction d’information, etc., ou les applications associées.
      L'atelier concerne aussi bien <u><b>l</b></u><u><b>es chercheurs
          du monde académique que ceux du secteur industriel</b></u>, et
      autant les notions conceptuelles que les applications. L'atelier
      est ouvert en termes de propositions. Nous souhaitons stimuler un
      échange et des discussions aussi bien du point de vue théorique
      qu'expérimental :<br>
      <ul>
        <li>    Quels succès ont été récemment rencontrés, et quels
          échecs ? </li>
        <li>    Quel est l'apport ?</li>
        <li>    Comment l’évaluer ?</li>
        <li>    Quid de l'explicabilité ? </li>
      </ul>
      Les présentations pourront concerner un travail abouti, des
      réflexions sur la modélisation ou  un travail préliminaire, ainsi
      que la réalisation de démonstrations.<br>
        <u><b><br>
        </b></u><u><b>Format des soumissions :</b></u><br>
      Nous proposons deux types de soumissions :<br>
      <ul>
        <li>    Soumission classique : article 12 pages maximum </li>
        <li>    Soumission courte : article 2 pages maximum</li>
      </ul>
      Les articles sont soumis suivant le format d'EGC. Les actes de
      l'atelier seront de plus mis à disposition sur le Web. <br>
       <br>
      <u><b>Dates importantes</b></u><br>
      <ul>
        <li>   Date limite de soumission des articles : 1er decembre :<b><s><span
                style="font-size:12.0pt;mso-bidi-font-size:
                10.0pt;font-family:&quot;New
                York&quot;,serif;mso-fareast-font-family:&quot;Times New
                Roman&quot;;
                mso-bidi-font-family:&quot;Times New
                Roman&quot;;mso-ansi-language:FR;mso-fareast-language:
                FR;mso-bidi-language:AR-SA;mso-no-proof:yes">27 novembre
                2022. <br>
              </span></s></b></li>
      </ul>
      <ul>
        <li>   Notification aux auteurs : 16 décembre.</li>
      </ul>
      <u><b>Lien de soumission :</b></u><br>
      <a class="moz-txt-link-freetext"
        href="https://easychair.org/my/conference?conf=m3a2023"
        moz-do-not-send="true">https://easychair.org/my/conference?conf=m3a2023</a><br>
      <p><u><b>Thèmes de l'atelier (liste non exhaustive) : </b></u></p>
      Une liste de thèmes est donnée ci-dessous, reste ouverte et est
      non limitative :<br>
      <ul>
        <li>    Réseaux de neurones et mécanismes d'attention (ex.
          transformers,...)</li>
        <li>    Capacité des architectures basées sur l'attention à
          encoder et extraire l'information pertinente</li>
        <li>    Combinaison des mécanismes d'attention et des approches
          classiques d'apprentissage</li>
        <li>    IA frugale et complexité des mécanismes d'attention </li>
        <li>    Adaptabilité des mécanismes d'attention à
          l'apprentissage fédéré </li>
        <li>    Explicabilité des modèles basés sur l'attention (lien
          avec les valeurs de Shapley...)</li>
        <li>    Les mécanismes d'attention dans les réseaux neuronaux
          profonds et leur explication</li>
        <li>    Transférabilité des connaissances dans les modèles basés
          sur l’attention </li>
        <li>    Données temporelles et mécanismes d'attention</li>
        <li>    Évaluation des valeurs générées par les mécanismes
          d'attention</li>
        <li>    Applications académiques et industrielles des modèles
          basés sur l'attention</li>
        <li>    Visualisation optimale des attentions</li>
      </ul>
      <u><b>Organisateurs </b></u><br>
      <ul>
        <li>    Mohamed-Djallel DILMI - LIPN, USPN-UPXIII</li>
        <li>    Florent FOREST - IMOS, EPFL</li>
        <li>    Hanene AZZAG - LIPN, USPN-UPXIII</li>
        <li>    Mustpha LEBBAH - David Lab, Université Paris Saclay -
          Campus UVSQ</li>
      </ul>
      <br>
    </blockquote>
    <pre class="moz-signature" cols="72">--
LIPN UMR 7030                             
Université Sorbonne Paris Nord        
99, avenue J.-B. Clement
93430 Villetaneuse</pre>
  </body>
</html>

--------------qO3Cs0p1sCXSG5944V2ZBqAp--
