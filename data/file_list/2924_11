Return-Path: <p_bruneau@hotmail.com>
X-Original-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Received: from BounceSmtp1.univ-nantes.fr (bouncesmtp1.u12.univ-nantes.prive [172.20.12.66])
	by sympa62.u12.univ-nantes.prive (Postfix) with ESMTP id 8AFA314017C0
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Mon,  7 Nov 2022 10:46:03 +0100 (CET)
Received: from mx2.localdomain (MX2.univ-nantes.fr [193.52.101.136])
	by BounceSmtp1.univ-nantes.fr (Postfix) with ESMTP id 8706C6747
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Mon,  7 Nov 2022 10:46:03 +0100 (CET)
Received: from localhost (localhost [127.0.0.1])
	by mx2.localdomain (Postfix) with ESMTP id 7ECE8101504
	for <liste-egc@polytech.univ-nantes.fr>; Mon,  7 Nov 2022 10:46:03 +0100 (CET)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: 0.899
X-Spam-Level:
X-Spam-Status: No, score=0.899 tagged_above=-1000 required=5
	tests=[CRM114_UNSURE=0.1, DKIM_SIGNED=0.1, DKIM_VALID=-0.1,
	DKIM_VALID_AU=-0.1, DKIM_VALID_EF=-0.1, HTML_MESSAGE=0.001,
	RCVD_IN_WSFF=0.01, SARE_HEAD_8BIT_SPAM=0.888, SPF_HELO_NONE=0.001,
	SPF_PASS=-0.001, UN_PHISHING_PW=0.1] autolearn=disabled
X-CRM114-Status: UNSURE ( 4.4653 )
X-CRM114-CacheID: 
Authentication-Results: univ-nantes.fr (amavisd-new); dkim=pass (2048-bit key)
	header.d=univ-paris13.fr
Received: from mx2.localdomain ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id Se8fcY14TxG9 for <liste-egc@polytech.univ-nantes.fr>;
	Mon,  7 Nov 2022 10:45:57 +0100 (CET)
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from smtp2.univ-paris13.fr (smtp2.univ-paris13.fr [81.194.43.176])
	by mx2.localdomain (Postfix) with ESMTPS id DA7B21014C2
	for <liste-egc@polytech.univ-nantes.fr>; Mon,  7 Nov 2022 10:45:57 +0100 (CET)
Received: from [10.10.1.213] (gw.lipn.univ-paris13.fr [194.254.163.15])
	(Authenticated sender: azzag)
	by smtp2.univ-paris13.fr (Postfix) with ESMTPSA id 94DEC2520595;
	Mon,  7 Nov 2022 10:45:56 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=univ-paris13.fr;
	s=dkim; t=1667814356;
	h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
	 to:to:cc:mime-version:mime-version:content-type:content-type:
	 in-reply-to:in-reply-to:references:references;
	bh=BPMPewLsSoQS+EFgOMgkUopRuTClJFiffA/JZLq6KGU=;
	b=jK+FlzeZ8MHkAhVe+BrZK8AGyCEMyfmh5hJONTwGqxxiv5m4d/8wV+AGByzat1Q47wA27y
	lCD/L5k/6G6MSGG2SkKadNWZLKF9v7z32+gFJjQz/wLb2indT3bregIqy3CYZpyqkg9cqZ
	yJRFjRFnn7tJCrFtYOIeyIJ7ZACrxjQPre0nrzmhtLg+s3pcnD/Ze2jk+JCAm81BJBlF6e
	epcImkGhNJQJqxBtu7ffvx2VvreoGvi8brUMkC51N4ZMlM5PS/Wwec5nRXHitvlZYe/zhM
	DQKzELWjpHvoa+JbYDXx48ly4WA+AAgltMlKeHAqhb3gSiiN9qRyUDsswHhhQw==
Content-Type: multipart/alternative;
 boundary="------------uqNGQKNmo6TvzoWypXopk18T"
Message-ID: <8edc4521-04fc-830e-db6f-60fdb201af3d@univ-paris13.fr>
Date: Mon, 7 Nov 2022 10:45:18 +0100
MIME-Version: 1.0
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101
 Thunderbird/102.4.1
Content-Language: fr
From: Hanane AZZAG <azzag@univ-paris13.fr>
To: liste-egc@polytech.univ-nantes.fr, bull-i3@irit.fr
References: <c4c5bf2b-721d-7458-c5da-361cf083f152@univ-paris13.fr>
In-Reply-To: <c4c5bf2b-721d-7458-c5da-361cf083f152@univ-paris13.fr>
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] Atelier =?UTF-8?Q?M=C3=A9canismes_d=E2=80=99Attention?=
 et Apprentissage Automatique : =?UTF-8?Q?avanc=C3=A9es_r=C3=A9centes?= et
 perspectives @EGC'23

This is a multi-part message in MIME format.
--------------uqNGQKNmo6TvzoWypXopk18T
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit

*****Atelier@EGC'23 : Mécanismes d'Attention et Apprentissage 
Automatique : avancées récentes et 
perspective**s***************************

*https://m3a2023.sciencesconf.org/*_*
*_

_*Présentation de l'atelier*_

Cet atelier a pour objectif de réunir les chercheurs intéressés par les 
mécanismes d’attentions et leur apport dans le domaine de 
l'apprentissage automatique. Notre ambition est de permettre aux 
participants d'aborder tous les thèmes allant de la modélisation de 
l’attention aux applications en passant par les architectures basées sur 
de tels mécanismes. Les problématiques abordées lors de cet atelier 
peuvent concerner les processus de modélisation, extraction 
d’information, etc., ou les applications associées. L'atelier concerne 
aussi bien _*l*__*es chercheurs du monde académique que ceux du secteur 
industriel*_, et autant les notions conceptuelles que les applications. 
L'atelier est ouvert en termes de propositions. Nous souhaitons stimuler 
un échange et des discussions aussi bien du point de vue théorique 
qu'expérimental :

  *      Quels succès ont été récemment rencontrés, et quels échecs ?
  *      Quel est l'apport ?
  *      Comment l’évaluer ?
  *      Quid de l'explicabilité ?

Les présentations pourront concerner un travail abouti, des réflexions 
sur la modélisation ou  un travail préliminaire, ainsi que la 
réalisation de démonstrations.
_*
*__*Format des soumissions :*_
Nous proposons deux types de soumissions :

  *      Soumission classique : article 12 pages maximum
  *      Soumission courte : article 2 pages maximum

Les articles sont soumis suivant le format d'EGC. Les actes de l'atelier 
seront de plus mis à disposition sur le Web.

_*Dates importantes*_

  *     Date limite de soumission des articles :*_27 novembre 2022._*
  *     Notification aux auteurs : 16 décembre.

_*Lien de soumission :*_
https://easychair.org/my/conference?conf=m3a2023

_*Thèmes de l'atelier (liste non exhaustive) : *_

Une liste de thèmes est donnée ci-dessous, reste ouverte et est non 
limitative :

  *      Réseaux de neurones et mécanismes d'attention (ex.
    transformers,...)
  *      Capacité des architectures basées sur l'attention à encoder et
    extraire l'information pertinente
  *      Combinaison des mécanismes d'attention et des approches
    classiques d'apprentissage
  *      IA frugale et complexité des mécanismes d'attention
  *      Adaptabilité des mécanismes d'attention à l'apprentissage fédéré
  *      Explicabilité des modèles basés sur l'attention (lien avec les
    valeurs de Shapley...)
  *      Les mécanismes d'attention dans les réseaux neuronaux profonds
    et leur explication
  *      Transférabilité des connaissances dans les modèles basés sur
    l’attention
  *      Données temporelles et mécanismes d'attention
  *      Évaluation des valeurs générées par les mécanismes d'attention
  *      Applications académiques et industrielles des modèles basés sur
    l'attention
  *      Visualisation optimale des attentions

_*Organisateurs *_

  *      Mohamed-Djallel DILMI - LIPN, USPN-UPXIII
  *      Florent FOREST - IMOS, EPFL
  *      Hanene AZZAG - LIPN, USPN-UPXIII
  *      Mustpha LEBBAH - David Lab, Université Paris Saclay - Campus UVSQ

-- 
--------------uqNGQKNmo6TvzoWypXopk18T
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: 8bit

<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>
  <body>
    <b>**</b><b>Atelier@EGC'23 : Mécanismes d'Attention et Apprentissage
      Automatique : avancées récentes et perspective</b><b>s</b><b>************************</b>
    <p><b><a class="moz-txt-link-freetext"
          href="https://m3a2023.sciencesconf.org/">https://m3a2023.sciencesconf.org/</a></b><u><b><br>
        </b></u></p>
    <p><u><b>Présentation de l'atelier</b></u></p>
    Cet atelier a pour objectif de réunir les chercheurs intéressés par
    les mécanismes d’attentions et leur apport dans le domaine de
    l'apprentissage automatique. Notre ambition est de permettre aux
    participants d'aborder tous les thèmes allant de la modélisation de
    l’attention aux applications en passant par les architectures basées
    sur de tels mécanismes. Les problématiques abordées lors de cet
    atelier peuvent concerner les processus de modélisation, extraction
    d’information, etc., ou les applications associées. L'atelier
    concerne aussi bien <u><b>l</b></u><u><b>es chercheurs du monde
        académique que ceux du secteur industriel</b></u>, et autant les
    notions conceptuelles que les applications. L'atelier est ouvert en
    termes de propositions. Nous souhaitons stimuler un échange et des
    discussions aussi bien du point de vue théorique qu'expérimental :<br>
    <ul>
      <li>    Quels succès ont été récemment rencontrés, et quels échecs
        ? </li>
      <li>    Quel est l'apport ?</li>
      <li>    Comment l’évaluer ?</li>
      <li>    Quid de l'explicabilité ? </li>
    </ul>
    Les présentations pourront concerner un travail abouti, des
    réflexions sur la modélisation ou  un travail préliminaire, ainsi
    que la réalisation de démonstrations.<br>
      <u><b><br>
      </b></u><u><b>Format des soumissions :</b></u><br>
    Nous proposons deux types de soumissions :<br>
    <ul>
      <li>    Soumission classique : article 12 pages maximum </li>
      <li>    Soumission courte : article 2 pages maximum</li>
    </ul>
    Les articles sont soumis suivant le format d'EGC. Les actes de
    l'atelier seront de plus mis à disposition sur le Web. <br>
     <br>
    <u><b>Dates importantes</b></u><br>
    <ul>
      <li>   Date limite de soumission des articles :<b> <u>27 novembre
            2022.</u></b></li>
      <li>   Notification aux auteurs : 16 décembre.</li>
    </ul>
    <u><b>Lien de soumission :</b></u><br>
    <a class="moz-txt-link-freetext"
      href="https://easychair.org/my/conference?conf=m3a2023">https://easychair.org/my/conference?conf=m3a2023</a><br>
    <p><u><b>Thèmes de l'atelier (liste non exhaustive) : </b></u></p>
    Une liste de thèmes est donnée ci-dessous, reste ouverte et est non
    limitative :<br>
    <ul>
      <li>    Réseaux de neurones et mécanismes d'attention (ex.
        transformers,...)</li>
      <li>    Capacité des architectures basées sur l'attention à
        encoder et extraire l'information pertinente</li>
      <li>    Combinaison des mécanismes d'attention et des approches
        classiques d'apprentissage</li>
      <li>    IA frugale et complexité des mécanismes d'attention </li>
      <li>    Adaptabilité des mécanismes d'attention à l'apprentissage
        fédéré </li>
      <li>    Explicabilité des modèles basés sur l'attention (lien avec
        les valeurs de Shapley...)</li>
      <li>    Les mécanismes d'attention dans les réseaux neuronaux
        profonds et leur explication</li>
      <li>    Transférabilité des connaissances dans les modèles basés
        sur l’attention </li>
      <li>    Données temporelles et mécanismes d'attention</li>
      <li>    Évaluation des valeurs générées par les mécanismes
        d'attention</li>
      <li>    Applications académiques et industrielles des modèles
        basés sur l'attention</li>
      <li>    Visualisation optimale des attentions</li>
    </ul>
    <u><b>Organisateurs </b></u><br>
    <ul>
      <li>    Mohamed-Djallel DILMI - LIPN, USPN-UPXIII</li>
      <li>    Florent FOREST - IMOS, EPFL</li>
      <li>    Hanene AZZAG - LIPN, USPN-UPXIII</li>
      <li>    Mustpha LEBBAH - David Lab, Université Paris Saclay -
        Campus UVSQ</li>
    </ul>
    --
  </body>
</html>

--------------uqNGQKNmo6TvzoWypXopk18T--
