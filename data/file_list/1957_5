Return-Path: <p_bruneau@hotmail.com>
X-Original-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Received: from BounceSmtp1.univ-nantes.fr (BounceSMTP1.univ-nantes.prive [172.20.12.66])
	by sympa62.u12.univ-nantes.prive (Postfix) with ESMTP id C4A4614017DA
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Fri, 28 Aug 2020 21:37:20 +0200 (CEST)
Received: from mx1.d101.univ-nantes.fr (MX1.univ-nantes.fr [193.52.101.135])
	by BounceSmtp1.univ-nantes.fr (Postfix) with ESMTP id BC0876D19FF
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Fri, 28 Aug 2020 21:37:20 +0200 (CEST)
Received: from localhost (localhost [127.0.0.1])
	by mx1.d101.univ-nantes.fr (Postfix) with ESMTP id B760542F200F
	for <polytech_liste-egc@univ-nantes.fr>; Fri, 28 Aug 2020 21:37:20 +0200 (CEST)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: 4.034
X-Spam-Level: ****
X-Spam-Status: No, score=4.034 tagged_above=-1000 required=5
	tests=[CRM114_UNSURE=0.1, DKIM_SIGNED=0.1, DKIM_VALID=-0.1,
	DKIM_VALID_AU=-0.1, DKIM_VALID_EF=-0.1, FREEMAIL_FROM=0.001,
	HTML_MESSAGE=0.001, MIME_HTML_ONLY=0.1, MR_NOT_ATTRIBUTED_IP=0.2,
	NO_RDNS2=0.01, RCVD_IN_PSBL=2.7, RCVD_IN_SORBS=1, RCVD_IN_WSFF=0.01,
	SPF_HELO_NONE=0.001, T_SPF_TEMPERROR=0.01, UN_PHISHING_PW=0.1,
	URIBL_BLOCKED=0.001] autolearn=disabled
X-CRM114-Status: UNSURE ( 1.3270 )
X-CRM114-CacheID: 
Authentication-Results: univ-nantes.fr (amavisd-new); dkim=pass (1024-bit key)
	header.d=gmx.net
Received: from mx1.d101.univ-nantes.fr ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id dhcrsFClqHNL for <polytech_liste-egc@univ-nantes.fr>;
	Fri, 28 Aug 2020 21:37:14 +0200 (CEST)
X-Greylist: delayed 00:20:04 by SQLgrey-1.6.7
Received: from mout-xforward.gmx.net (mout-xforward.gmx.net [82.165.159.42])
	by mx1.d101.univ-nantes.fr (Postfix) with ESMTPS id 7103841779A4
	for <polytech_liste-egc@univ-nantes.fr>; Fri, 28 Aug 2020 21:37:14 +0200 (CEST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=gmx.net;
	s=badeba3b8450; t=1598643434;
	bh=oNJ9ZlZQkRnp1LCkUSwE4Z9PdLQr9Sgcuyc2ZFEvuc8=;
	h=X-UI-Sender-Class:From:To:Subject:Date;
	b=Y7C6WUxh+/3i2hKL5i+e3AgFOU5Ylbgf4u2RqijPzQ8H9eq6PKmjQBlHh9KvZGF3Z
	 N/ny3qVE2YVblG/Xcf34O4P6385jo1IRUCHpxBh9yNjqd0VkPQ4ObPFLbCnM1V76qL
	 LXBhQQ+wDzd15xD8fR8zAGIBimuq2lTtyL+R/+so=
X-UI-Sender-Class: 01bb95c1-4bf8-414a-932a-4f6e2808ef9c
Received: from [91.160.101.96] ([91.160.101.96]) by web-mail.gmx.net
 (3c-app-gmx-bap78.server.lan [172.19.172.136]) (via HTTP); Fri, 28 Aug 2020
 21:17:04 +0200
MIME-Version: 1.0
Message-ID: <trinity-59e95f94-bc4b-4c8f-a1fd-4a0433d05d56-1598642224091@3c-app-gmx-bap78>
From: Albrecht Zimmermann <Albrecht_Zimmermann@gmx.net>
To: Liste EGC <polytech_liste-egc@univ-nantes.fr>, liste-proml@lri.fr,
 connectionists@mailman.srv.cs.cmu.edu
Content-Type: text/html; charset=UTF-8
Date: Fri, 28 Aug 2020 21:17:04 +0200
Importance: normal
Sensitivity: Normal
X-Priority: 3
X-Provags-ID: V03:K1:g2qJgNzso22aBPZ09/kVnIihfmXbBdb8i9mwKOTJz8eAFC6FrGcddRszehgyaApVUCvGD
 qF0+jn/n5kUVDqgn4/Aa/+yjya9br+5bnRpWz1umVFD49gG+g8KGnjTvykFNqzzaOd9iu2SjCv5j
 i+NADk0p3FUKUIilMsVbkaiOrZ1NwqMkm9SyWKba1oj1w3is8OzI+FaxVpMBuT2Jiwlbiw3iTNEn
 w3Fa70ZNx451P1fbIh0BKkLA3XZwNRLWKLqiLTS0jYcKkoPOPdgPjpFoJMmkBJTxp6vWDCc3pSOs
 sk=
X-UI-Out-Filterresults: junk:10;V03:K0:34vm9xslxOw=:1ybAMw5znaqiVqK2uUx3+E13
 IsXuRCeS6UZhyGPxEmo5B6Ij2O6YD/Y3jk+ufMpc8zD/6ZRaK39HhsdJy52FObMm//54iMLkk
 iDS1qAAXN9IbwICpBucF0H4zY+K9NuRJmTJpy6r/3461Ua8nXwWioxrs59Le6OPeinhPEiw0P
 hcfIkWjGYq3vX6MYBKdvXcs7ypZSH6KVFqY9IdMzp17Y5TmClGYwgUD0OH/FHNCkpk3D65vls
 kJTIaAjk/XygIQ5LMZ4tQbSz3bTxRQjcITB3psftnpcZkQLBB8Gf1vqJd4jfabbWA+WdjDqbZ
 8nY+WrBwvz1TopAZxZ/5zrvBPCpCWwVq2feVnKVTDdCgTZKV+oHBTBuiQD7jfF59NjRZkVWd3
 b1qKpIVm1xqrGFEYtw5aO9TruGy4jt+3KzMc0VQy7hegjqQBzeeL7YT9CKpL6neQp1y2KM245
 K6DGZjTOOLbtaIW6IuL/ra8wlI6rsoyZXtehW4+lrdjuAiV/VI4NrDPJI5fKv9pwJt9lT2Cmh
 9RuxmzZg7w+EYsixIf1McWSPyk9vYWASkUOL1PPGkWxR7a0RbjxrVJ5A5GxQ7fH8AVIaQBtHa
 V9FT8NMKxVT+FmftCr7MTHU71g61sAnnoTIoIGbtrHmntGARAGXvF/LO4QCtlR+8Fml546Tmj
 z1EhkWRbtfzZbyeMdeaVJePYQlmRzd0nrAjm8+f/DMXk/VTvo/DkgLpXKfxfy2wYSLOTdWLsL
 iL87KJt4Nqfdjio/27lcfjbPL9OplLy/8PbQ==
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] Call for Papers -- Big Data Journal Special Issue on
 Evaluation and Experimental Design in Data Mining and Machine Learning

<html><head></head><body><div style="font-family: Verdana;font-size: 12.0px;"><div>*Submission Deadline:* February 1, 2021<br/>
*Special Issue Publication Date:* Late 2021<br/>
*URL*: https://home.liebertpub.com/cfp/special-issue-on-evaluation-and-experimental-design-in-data-/283/</div>

<div><br/>
*Guest Editors:*</div>

<div>Eirini Ntoutsi, Associate Professor, Leibniz University Hannover &amp; L3S Research Center, Germany<br/>
Erich Schubert, Associate Professor, Technical University Dortmund, Germany<br/>
Arthur Zimek, Professor, University of Southern Denmark, Denmark<br/>
Albrecht Zimmermann, Associate Professor, University Caen Normandy, France<br/>
&nbsp;</div>

<div>A vital part of proposing new machine learning and data mining approaches is evaluating them empirically to allow an assessment of their capabilities. Numerous choices go into setting up such experiments: how to choose the data, how to preprocess them (or not), potential problems associated with the selection of datasets, what other techniques to compare to (if any), what metrics to evaluate, etc. and last but not least how to present and interpret the results. Learning how to make those choices on-the-job, often by copying the evaluation protocols used in the existing literature, can easily lead to the development of problematic habits. Numerous, albeit scattered, publications have called attention to those questions and have occasionally called into question published results, or the usability of published methods.</div>

<div>Those studies consider different evaluation aspects in isolation, and the issue becomes even more complex because setting up an experiment introduces additional dependencies and biases: having chosen an evaluation metric with little bias can be easily undermined choosing data that cannot appropriately treated by one of the comparison techniques, for instance, and having carefully addressed both aspects is of little worth if the statistical test chosen does not allow to assess significance.</div>

<div>At a time of intense discussions about a reproducibility crisis in natural, social, and life sciences, and conferences such as SIGMOD, KDD, and ECML/PKDD encouraging researchers to make their work as reproducible as possible, we therefore feel that it is important to discuss those issues on a fundamental level. In non-computational sciences, experimental design has been studied in depth, which has given rise to such principles as randomization, blocking, or factorial experiments. While these principles are usually not applied in machine learning and data mining, one desirable goal might be the formulation of a checklist that quickly allows to evaluate the experiment one is about to perform, and to identify and correct weaknesses. An important starting point of any such list has to be: &ldquo;What question do we want to answer?&rdquo;</div>

<div>An issue directly related to the dataset choice mentioned above is the following: even the best-designed experiment carries only limited information if the underlying data are lacking. We therefore also want to discuss questions related to the availability of data, whether they are reliable, diverse, and whether they correspond to realistic and/or challenging problem settings. This is of particular importance because our field is at a disadvantage compared to other experimental science: whereas there, data are collected (e.g., in social sciences), or generated (e.g., in physics), we often &ldquo;only&rdquo; use existing data.</div>

<div>Finally, we want to emphasize the responsibility of the researchers to communicate their research as objectively as possible. We also want to highlight the critical role of the reviewers: The typical expectation of many reviewers seems to be that an evaluation should demonstrate that a newly proposed method is better than existing work. This can be shown on a few example datasets at most and is still not necessarily true in general. Rather it should be demonstrated in papers (and appreciated by reviewers) to show on what kind of data a new method works well, and also where it does not, and this way in which respect it is different from existing work and therefore is a useful complement. A related topic is therefore also how to characterize datasets, e.g., in terms of their learning complexity and how to create benchmark datasets, an essential tool for method development and assessment, adopted by other domains like computer vision, IR etc.</div>

<div>*Topics for this special issue:*</div>

<div>For this special issue, we mainly solicit contributions that discuss those questions on a fundamental level, take stock of the state-of-the-art, offer theoretical arguments, or take well-argued positions, as well as actual evaluation papers that offer new insights, e.g. question published results, or shine the spotlight on the characteristics of existing benchmark data sets.</div>

<div>As such, topics include, but are not limited to:<br/>
&nbsp;<br/>
- Benchmark datasets for data mining tasks: are they diverse/realistic/challenging?<br/>
- Impact of data quality (redundancy, errors, noise, bias, imbalance, ...) on qualitative evaluation<br/>
- Propagation/amplification of data quality issues on the data mining results (also interplay between data and algorithms)<br/>
- Evaluation of unsupervised data mining (dilemma between novelty and validity)<br/>
- Evaluation measures<br/>
- (Automatic) data quality evaluation tools: What are the aspects one should check before starting to apply algorithms to given data?<br/>
- Issues around runtime evaluation (algorithm vs. implementation, dependency on hardware, algorithm parameters, dataset characteristics)<br/>
- Design guidelines for crowd-sourced evaluations<br/>
- Principled experimental workflows</div>

<div>Following two workshops on Experimental Design in Data Mining and Machine Learning (EDML), we invite now papers on these topics for a special issue of Big Data. While extended versions of previous EDML workshop papers are welcome, we also openly invite new submissions that are independent of the EDML workshops.</div>

<div>Please direct special issue inquiries to Arthur Zimek</div></div></body></html>
