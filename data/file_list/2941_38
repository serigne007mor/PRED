Return-Path: <p_bruneau@hotmail.com>
X-Original-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Received: from bouncesmtp2.univ-nantes.fr (bouncesmtp2.u12.univ-nantes.prive [172.20.12.67])
	by sympa62.u12.univ-nantes.prive (Postfix) with ESMTP id D75C214017DD
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Fri, 18 Nov 2022 15:15:53 +0100 (CET)
Received: from mx1.localdomain (MX1.univ-nantes.fr [193.52.101.135])
	by bouncesmtp2.univ-nantes.fr (Postfix) with ESMTP id D37DC6663
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Fri, 18 Nov 2022 15:15:53 +0100 (CET)
Received: from localhost (localhost [127.0.0.1])
	by mx1.localdomain (Postfix) with ESMTP id C8BCE120D82
	for <liste-egc@polytech.univ-nantes.fr>; Fri, 18 Nov 2022 15:15:53 +0100 (CET)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: 0.9
X-Spam-Level:
X-Spam-Status: No, score=0.9 tagged_above=-1000 required=5
	tests=[CRM114_UNSURE=0.1, DKIM_SIGNED=0.1, DKIM_VALID=-0.1,
	DKIM_VALID_AU=-0.1, DKIM_VALID_EF=-0.1, HTML_MESSAGE=0.001,
	RCVD_IN_WSFF=0.01, SARE_HEAD_8BIT_SPAM=0.888, SPF_HELO_NONE=0.001,
	SPF_PASS=-0.001, UN_PHISHING_PW=0.1, URIBL_BLOCKED=0.001]
	autolearn=disabled
X-CRM114-Status: UNSURE ( 4.6268 )
X-CRM114-CacheID: 
Authentication-Results: univ-nantes.fr (amavisd-new); dkim=pass (2048-bit key)
	header.d=univ-paris13.fr
Received: from mx1.localdomain ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id 9SbxJUv_ArM7 for <liste-egc@polytech.univ-nantes.fr>;
	Fri, 18 Nov 2022 15:15:51 +0100 (CET)
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from smtp2.univ-paris13.fr (smtp2.univ-paris13.fr [81.194.43.176])
	by mx1.localdomain (Postfix) with ESMTPS id 73CEF120040
	for <liste-egc@polytech.univ-nantes.fr>; Fri, 18 Nov 2022 15:15:51 +0100 (CET)
Received: from [10.10.1.213] (gw.lipn.univ-paris13.fr [194.254.163.15])
	(Authenticated sender: azzag)
	by smtp2.univ-paris13.fr (Postfix) with ESMTPSA id 482702520375;
	Fri, 18 Nov 2022 15:15:50 +0100 (CET)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=univ-paris13.fr;
	s=dkim; t=1668780950;
	h=from:from:reply-to:subject:subject:date:date:message-id:message-id:
	 to:to:cc:mime-version:mime-version:content-type:content-type:
	 in-reply-to:in-reply-to:references:references;
	bh=S+yxVucEA5/buPqHBu4zd9mIg6ZbtKzb4SZ8JK1xrt0=;
	b=l3BkiQjwhPyTqIR3+YZTBdNegaWOqVDFYc4bHsfmweEjjd/RPfAZKU1t02c8GHlVxVlkXE
	ZluemNJ2D/5qGr9EsqSZfudsRWvtA5EHrUFWz7RsveMNPpgC+M4czBtVM6MOStPLSyLo7j
	bsupKnmoRqtjJCHjvul96xZgtGu9n6oM5iLcKNYjnQQgKE3XRB6uX2MOYYm6YAeeOHWQz7
	iVokOfarNak2ZqzrLxQMpfWf4khMvSHgMx6C3jP7koXSoYxT16pwm0xE91PdvIH2bhbBHt
	aoz1TXcO8c+rTac5R3xyVB9X8ia0ICa/sztTuMHAq0kntvxkoR/ogggoMmcZfA==
Content-Type: multipart/alternative;
 boundary="------------VG852MEtSgB0cZl01WAlhECV"
Message-ID: <0eb9c819-58cb-4f65-bee6-0aa2736111ee@univ-paris13.fr>
Date: Fri, 18 Nov 2022 15:15:07 +0100
MIME-Version: 1.0
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:102.0) Gecko/20100101
 Thunderbird/102.4.2
Content-Language: fr
References: <c4c5bf2b-721d-7458-c5da-361cf083f152@univ-paris13.fr>
To: liste-egc@polytech.univ-nantes.fr, bull-i3@irit.fr
From: Hanane AZZAG <azzag@univ-paris13.fr>
In-Reply-To: <c4c5bf2b-721d-7458-c5da-361cf083f152@univ-paris13.fr>
X-Forwarded-Message-Id: <c4c5bf2b-721d-7458-c5da-361cf083f152@univ-paris13.fr>
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] Atelier =?UTF-8?Q?M=C3=A9canismes_d=E2=80=99Attention?=
 et Apprentissage Automatique : =?UTF-8?Q?avanc=C3=A9es_r=C3=A9centes?= et
 perspectives @EGC'23

This is a multi-part message in MIME format.
--------------VG852MEtSgB0cZl01WAlhECV
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit


***********************************Atelier@EGC'23 : Mécanismes 
d'Attention et Apprentissage Automatique : avancées récentes et 
perspective**s***************************

*https://m3a2023.sciencesconf.org/*_*
*_

_*Présentation de l'atelier*_

Cet atelier a pour objectif de réunir les chercheurs intéressés par les 
mécanismes d’attentions et leur apport dans le domaine de 
l'apprentissage automatique. Notre ambition est de permettre aux 
participants d'aborder tous les thèmes allant de la modélisation de 
l’attention aux applications en passant par les architectures basées sur 
de tels mécanismes. Les problématiques abordées lors de cet atelier 
peuvent concerner les processus de modélisation, extraction 
d’information, etc., ou les applications associées. L'atelier concerne 
aussi bien _*l*__*es chercheurs du monde académique que ceux du secteur 
industriel*_, et autant les notions conceptuelles que les applications. 
L'atelier est ouvert en termes de propositions. Nous souhaitons stimuler 
un échange et des discussions aussi bien du point de vue théorique 
qu'expérimental :

  *      Quels succès ont été récemment rencontrés, et quels échecs ?
  *      Quel est l'apport ?
  *      Comment l’évaluer ?
  *      Quid de l'explicabilité ?

Les présentations pourront concerner un travail abouti, des réflexions 
sur la modélisation ou  un travail préliminaire, ainsi que la 
réalisation de démonstrations.
_*
*__*Format des soumissions :*_
Nous proposons deux types de soumissions :

  *      Soumission classique : article 12 pages maximum
  *      Soumission courte : article 2 pages maximum

Les articles sont soumis suivant le format d'EGC. Les actes de l'atelier 
seront de plus mis à disposition sur le Web.

_*Dates importantes*_

  *     Date limite de soumission des articles :*_27 novembre 2022._*
  *     Notification aux auteurs : 16 décembre.

_*Lien de soumission :*_
https://easychair.org/my/conference?conf=m3a2023

_*Thèmes de l'atelier (liste non exhaustive) : *_

Une liste de thèmes est donnée ci-dessous, reste ouverte et est non 
limitative :

  *      Réseaux de neurones et mécanismes d'attention (ex.
    transformers,...)
  *      Capacité des architectures basées sur l'attention à encoder et
    extraire l'information pertinente
  *      Combinaison des mécanismes d'attention et des approches
    classiques d'apprentissage
  *      IA frugale et complexité des mécanismes d'attention
  *      Adaptabilité des mécanismes d'attention à l'apprentissage fédéré
  *      Explicabilité des modèles basés sur l'attention (lien avec les
    valeurs de Shapley...)
  *      Les mécanismes d'attention dans les réseaux neuronaux profonds
    et leur explication
  *      Transférabilité des connaissances dans les modèles basés sur
    l’attention
  *      Données temporelles et mécanismes d'attention
  *      Évaluation des valeurs générées par les mécanismes d'attention
  *      Applications académiques et industrielles des modèles basés sur
    l'attention
  *      Visualisation optimale des attentions

_*Organisateurs *_

  *      Mohamed-Djallel DILMI - LIPN, USPN-UPXIII
  *      Florent FOREST - IMOS, EPFL
  *      Hanene AZZAG - LIPN, USPN-UPXIII
  *      Mustpha LEBBAH - David Lab, Université Paris Saclay - Campus UVSQ

--
--------------VG852MEtSgB0cZl01WAlhECV
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: 8bit

<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  </head>
  <body>
    <br>
    <div class="moz-forward-container"><b>********************************</b><b>Atelier@EGC'23
        : Mécanismes d'Attention et Apprentissage Automatique : avancées
        récentes et perspective</b><b>s</b><b>************************</b>
      <p><b><a class="moz-txt-link-freetext"
            href="https://m3a2023.sciencesconf.org/"
            moz-do-not-send="true">https://m3a2023.sciencesconf.org/</a></b><u><b><br>
          </b></u></p>
      <p><u><b>Présentation de l'atelier</b></u></p>
      Cet atelier a pour objectif de réunir les chercheurs intéressés
      par les mécanismes d’attentions et leur apport dans le domaine de
      l'apprentissage automatique. Notre ambition est de permettre aux
      participants d'aborder tous les thèmes allant de la modélisation
      de l’attention aux applications en passant par les architectures
      basées sur de tels mécanismes. Les problématiques abordées lors de
      cet atelier peuvent concerner les processus de modélisation,
      extraction d’information, etc., ou les applications associées.
      L'atelier concerne aussi bien <u><b>l</b></u><u><b>es chercheurs
          du monde académique que ceux du secteur industriel</b></u>, et
      autant les notions conceptuelles que les applications. L'atelier
      est ouvert en termes de propositions. Nous souhaitons stimuler un
      échange et des discussions aussi bien du point de vue théorique
      qu'expérimental :<br>
      <ul>
        <li>    Quels succès ont été récemment rencontrés, et quels
          échecs ? </li>
        <li>    Quel est l'apport ?</li>
        <li>    Comment l’évaluer ?</li>
        <li>    Quid de l'explicabilité ? </li>
      </ul>
      Les présentations pourront concerner un travail abouti, des
      réflexions sur la modélisation ou  un travail préliminaire, ainsi
      que la réalisation de démonstrations.<br>
        <u><b><br>
        </b></u><u><b>Format des soumissions :</b></u><br>
      Nous proposons deux types de soumissions :<br>
      <ul>
        <li>    Soumission classique : article 12 pages maximum </li>
        <li>    Soumission courte : article 2 pages maximum</li>
      </ul>
      Les articles sont soumis suivant le format d'EGC. Les actes de
      l'atelier seront de plus mis à disposition sur le Web. <br>
       <br>
      <u><b>Dates importantes</b></u><br>
      <ul>
        <li>   Date limite de soumission des articles :<b> <u>27
              novembre 2022.</u></b></li>
        <li>   Notification aux auteurs : 16 décembre.</li>
      </ul>
      <u><b>Lien de soumission :</b></u><br>
      <a class="moz-txt-link-freetext"
        href="https://easychair.org/my/conference?conf=m3a2023"
        moz-do-not-send="true">https://easychair.org/my/conference?conf=m3a2023</a><br>
      <p><u><b>Thèmes de l'atelier (liste non exhaustive) : </b></u></p>
      Une liste de thèmes est donnée ci-dessous, reste ouverte et est
      non limitative :<br>
      <ul>
        <li>    Réseaux de neurones et mécanismes d'attention (ex.
          transformers,...)</li>
        <li>    Capacité des architectures basées sur l'attention à
          encoder et extraire l'information pertinente</li>
        <li>    Combinaison des mécanismes d'attention et des approches
          classiques d'apprentissage</li>
        <li>    IA frugale et complexité des mécanismes d'attention </li>
        <li>    Adaptabilité des mécanismes d'attention à
          l'apprentissage fédéré </li>
        <li>    Explicabilité des modèles basés sur l'attention (lien
          avec les valeurs de Shapley...)</li>
        <li>    Les mécanismes d'attention dans les réseaux neuronaux
          profonds et leur explication</li>
        <li>    Transférabilité des connaissances dans les modèles basés
          sur l’attention </li>
        <li>    Données temporelles et mécanismes d'attention</li>
        <li>    Évaluation des valeurs générées par les mécanismes
          d'attention</li>
        <li>    Applications académiques et industrielles des modèles
          basés sur l'attention</li>
        <li>    Visualisation optimale des attentions</li>
      </ul>
      <u><b>Organisateurs </b></u><br>
      <ul>
        <li>    Mohamed-Djallel DILMI - LIPN, USPN-UPXIII</li>
        <li>    Florent FOREST - IMOS, EPFL</li>
        <li>    Hanene AZZAG - LIPN, USPN-UPXIII</li>
        <li>    Mustpha LEBBAH - David Lab, Université Paris Saclay -
          Campus UVSQ</li>
      </ul>
      --<br>
    </div>
  </body>
</html>

--------------VG852MEtSgB0cZl01WAlhECV--
