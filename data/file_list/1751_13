Return-Path: <mroche@lirmm.fr>
X-Original-To: polytech_liste-egc@sympa.univ-nantes.fr
Delivered-To: polytech_liste-egc@sympa.univ-nantes.fr
Received: from BounceSmtp1.univ-nantes.fr (BounceSMTP1.univ-nantes.prive [172.20.12.66])
	by Sympa.univ-nantes.fr (Postfix) with ESMTP id D7F351C14C67
	for <polytech_liste-egc@sympa.univ-nantes.fr>; Tue, 23 Jan 2007 19:36:44 +0100 (CET)
Received: from MX3.univ-nantes.fr (MX3.univ-nantes.fr [193.52.101.137])
	by BounceSmtp1.univ-nantes.fr (Postfix) with ESMTP id C74491060C
	for <polytech_liste-egc@sympa.univ-nantes.fr>; Tue, 23 Jan 2007 19:36:44 +0100 (CET)
Received: from localhost (debian [127.0.0.1])
	by MX3.univ-nantes.fr (Postfix) with ESMTP id B89A31C00940
	for <polytech_liste-egc@sympa.univ-nantes.fr>; Tue, 23 Jan 2007 19:36:44 +0100 (CET)
Received: from MX3.univ-nantes.fr ([193.52.101.137])
	by localhost (MX3.univ-nantes.fr [193.52.101.137]) (amavisd-new, port 10024)
	with LMTP id 26310-01-77
	for <polytech_liste-egc@sympa.univ-nantes.fr>;
	Tue, 23 Jan 2007 19:36:40 +0100 (CET)
Received: from lirmm.lirmm.fr (lirmm.lirmm.fr [193.49.104.10])
	by MX3.univ-nantes.fr (Postfix) with ESMTP id 555421C008CE
	for <liste-egc@polytech.univ-nantes.fr>; Tue, 23 Jan 2007 19:36:40 +0100 (CET)
Received: from localhost (spam.lirmm.fr [193.49.104.9])
	by lirmm.lirmm.fr (8.13.6/8.13.6/jlo-Mai2006) with ESMTP id l0NIadCL017555;
	Tue, 23 Jan 2007 19:36:39 +0100 (MET)
Received: from lirmm.lirmm.fr ([193.49.104.10])
 by localhost (spam.lirmm.fr [193.49.104.9]) (amavisd-new, port 10024)
 with LMTP id 25077-08; Tue, 23 Jan 2007 19:36:31 +0100 (CET)
Received: from www.lirmm.fr (janela.lirmm.fr [193.49.104.249])
	by lirmm.lirmm.fr (8.13.6/8.13.6/jlo-Mai2006) with ESMTP id l0NIaNKE017525;
	Tue, 23 Jan 2007 19:36:23 +0100 (MET)
Received: from 82.224.141.174
        (SquirrelMail authenticated user mroche)
        by www.lirmm.fr with HTTP;
        Tue, 23 Jan 2007 19:36:23 +0100 (CET)
Message-ID: <1342.82.224.141.174.1169577383.squirrel@www.lirmm.fr>
Date: Tue, 23 Jan 2007 19:36:23 +0100 (CET)
Subject: Appel : DEFT'07
From: Mathieu.Roche@lirmm.fr
To: liste-egc@polytech.univ-nantes.fr
User-Agent: SquirrelMail/1.5.1 [CVS]
MIME-Version: 1.0
Content-Type: text/plain;charset=iso-8859-1
Content-Transfer-Encoding: 8bit
X-Greylist: Sender DNS name whitelisted, not delayed by milter-greylist-1.6 (lirmm.lirmm.fr [193.49.104.10]); Tue, 23 Jan 2007 19:36:23 +0100 (MET)
X-Virus-Scanned: by amavisd-new/ClamAV/SA {jlo-09/02/06} at spam.lirmm.fr
X-Virus-Scanned: by amavisd-new-20030616-p7 (Debian) at univ-nantes.fr
X-CRM114-Version: 20040816.BlameClockworkOrange-auto.3 (regex: TRE TRE 0.7.2 (GPL)) MF-A10FFB4C 
X-CRM114-Status: Good  ( pR: 5.4262 )
X-Spam-Status: No, hits=-1.4 tagged_above=-10000.0 required=5.0
	tests=MANGLED_VIDEO, NO_RDNS2, NO_REAL_NAME, CRM_HAM_04
X-Spam-Level: 
X-Validation-by: fabrice.guillet@univ-nantes.fr

******************************************************************

DEFT'07 : Appel à participation

Atelier d'évaluation en fouille de textes sur la classification de textes
d'opinions

http://deft07.limsi.fr/

******************************************************************
Dates importantes :
- Inscription : à partir du 18 décembre 2006
- Diffusion des corpus d'apprentissage : 4 janvier 2007
- Test : 3 jours pris dans la dernière quinzaine de mars 2007
- Atelier : le 3 juillet lors de la plateforme de l'AFIA
******************************************************************

L'intérêt d'une conférence d'évaluation est de permettre de
confronter, sur un même corpus, des méthodes et logiciels d'équipes
différentes. Depuis deux ans, l'atelier d'évaluation DEFT propose des
thèmes relevant de la fouille de textes en langue française.  Les deux
éditions précédentes ont été consacrées à l'identification du locuteur
d'un discours (DEFT'05 http://www.lri.fr/ia/fdt/DEFT05/) et à la
segmentation thématique de textes (DEFT'06
http://www.lri.fr/ia/fdt/DEFT06/).

DEFT'07 se tiendra début juillet dans le cadre de la plateforme de l'AFIA
(http://afia2007.imag.fr/). Le thème de cette nouvelle édition est la
classification de textes d'opinion.

Un texte d'opinion présente un avis argumenté, positif ou négatif, sur un
sujet donné. Les domaines faisant l'objet de textes d'opinions sont
nombreux : critiques de films ou de livres, jugements qualitatifs de
produits, controverses sur un projet politique ... les exemples ne
manquent pas. Est-il possible de classer automatiquement un texte
d'opinion suivant le jugement, favorable ou défavorable, qu'il exprime ?
C'est l'enjeu du défi que nous proposons.

Pour ce défi, nous avons choisi des textes d'opinion venant de
différents domaines :

    * les critiques de films, livres, spectacles et BD ;
    * les tests de jeux vidéo ;
    * les interventions des parlementaires et du gouvernement
      dans les débats sur les projets de lois votés à l'Assemblée
      nationale.

Ces textes présentent la particularité d'être associés d'emblée à un
jugement exprimé sous la forme d'une note ou d'un vote. Ce sont ces
jugements qui serviront de référence lors de l'évaluation des
résultats. A partir de ces jugements, nous avons défini pour chaque corpus
un ensemble de classes d'opinion :

    * les classes bien, moyen, mauvais, pour les corpus sur les films,
      livres, spectacles, BD et sur les jeux vidéos,
    * les classes pour et contre pour le corpus sur les projets de lois.

La tâche des participants à DEFT'07 consistera à attribuer
automatiquement une classe d'opinion à chaque texte - critique, test, ou
intervention - de chaque corpus.

Les équipes participant à DEFT'07 devront s'inscrire à l'aide du
formulaire en ligne, et signer les accords de restriction d'usage des
corpus (http://deft07.limsi.fr/inscription.php).

Des corpus d'apprentissage seront fournis aux participants inscrits, à
partir du 4 janvier 2007. Ces corpus sont composés de 60% des corpus
d'origine. Ils contiennent la classe attribuée à chaque texte. Les
participants auront environ deux mois pour mettre en place leurs
méthodes de classification sur les corpus d'apprentissage. Seuls les
corpus d'apprentissage fournis sont autorisés pour l'entraînement à la
tâche.

Les 40% de corpus restants seront utilisés pour le test. Le test aura lieu
sur une fenêtre de 15 jours, à partir de la mi-mars. A partir de la date
qu'ils auront choisie dans cet intervalle, les participants auront trois
jours pour appliquer, sur les corpus de test, les
méthodes mises en oeuvre sur les corpus d'apprentissage.

****************************************************************** Comités :

Comité d'organisation :
Co-responsables : Thomas Heitz (LRI) et Martine Hurault-Plantet
(LIMSI)
Membres : Jean-Baptiste Berthelin (LIMSI), Sarra El Ayari (LIMSI), Cyril
Grouin (LIMSI), Michèle Jardino (LIMSI), Zohra Khalis
(Epigénomique), et Michel Lastes (LIMSI), webmestre

Comité de programme :
Co-présidents : Benoît Habert (LIMSI), Patrick Paroubek (LIMSI), et
Violaine Prince (LIRMM)
Membres : Nathalie Aussenac-Gilles (IRIT), Catherine Berrut (CLIPS),
Fabrice Clérot (France Telecom), Guillaume Cleuziou (LIFO), Béatrice
Daille (LINA), Marc El-Bèze (LIA), Patrick Gallinari (LIP6), Éric
Gaussier (Xerox Research), Thierry Hamon (LIPN), Fidélia
Ibekwe-SanJuan (URSIDOC-SII), Éric Laporte (IGM-LabInfo), Pascal
Poncelet (LGI2P), Christophe Roche (LISTIC), Mathieu Roche (LIRMM),
Pascale Sébillot (IRISA), Yannick Toussaint (LORIA), François Yvon (ENST).

******************************************************************



