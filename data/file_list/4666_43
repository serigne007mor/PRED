Return-Path: <p_bruneau@hotmail.com>
X-Original-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Delivered-To: polytech_liste-egc@sympa62.u12.univ-nantes.prive
Received: from bouncesmtp2.univ-nantes.fr (BounceSMTP2.univ-nantes.prive [172.20.12.67])
	by sympa62.u12.univ-nantes.prive (Postfix) with ESMTP id F2ED714006DE
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Sat, 16 May 2020 10:43:19 +0200 (CEST)
Received: from mx1.d101.univ-nantes.fr (MX1.univ-nantes.fr [193.52.101.135])
	by bouncesmtp2.univ-nantes.fr (Postfix) with ESMTP id F13D361F0E2
	for <polytech_liste-egc@sympa62.u12.univ-nantes.prive>; Sat, 16 May 2020 10:43:19 +0200 (CEST)
Received: from localhost (localhost [127.0.0.1])
	by mx1.d101.univ-nantes.fr (Postfix) with ESMTP id ED1B141BBD3E
	for <polytech_liste-egc@univ-nantes.fr>; Sat, 16 May 2020 10:43:19 +0200 (CEST)
X-Virus-Scanned: Debian amavisd-new at univ-nantes.fr
X-Spam-Flag: NO
X-Spam-Score: 0.322
X-Spam-Level:
X-Spam-Status: No, score=0.322 tagged_above=-1000 required=5
	tests=[CRM114_UNSURE=0.1, DKIM_SIGNED=0.1, DKIM_VALID=-0.1,
	DKIM_VALID_AU=-0.1, DKIM_VALID_EF=-0.1, FREEMAIL_FROM=0.001,
	HTML_MESSAGE=0.001, MIME_HTML_ONLY=0.1, MR_NOT_ATTRIBUTED_IP=0.2,
	NO_RDNS2=0.01, RCVD_IN_DNSWL_LOW=-1, RCVD_IN_MSPIKE_H2=-0.001,
	RCVD_IN_SORBS=1, RCVD_IN_WSFF=0.01, SPF_HELO_NONE=0.001,
	SPF_PASS=-0.001, UN_PHISHING_PW=0.1, URIBL_BLOCKED=0.001]
	autolearn=disabled
X-CRM114-Status: UNSURE ( 1.8706 )
X-CRM114-CacheID: 
Authentication-Results: univ-nantes.fr (amavisd-new); dkim=pass (1024-bit key)
	header.d=gmx.net
Received: from mx1.d101.univ-nantes.fr ([127.0.0.1])
	by localhost (univ-nantes.fr [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id nrQSPtcRsvPo for <polytech_liste-egc@univ-nantes.fr>;
	Sat, 16 May 2020 10:43:17 +0200 (CEST)
X-Greylist: domain auto-whitelisted by SQLgrey-1.6.7
Received: from mout.gmx.net (mout.gmx.net [212.227.15.15])
	by mx1.d101.univ-nantes.fr (Postfix) with ESMTPS id 40D3541BBD07
	for <polytech_liste-egc@univ-nantes.fr>; Sat, 16 May 2020 10:43:17 +0200 (CEST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=gmx.net;
	s=badeba3b8450; t=1589618579;
	bh=V0TF6LqqAzoSuN/VgIqk0qWJANU+u5hmIJPMEYoKXGM=;
	h=X-UI-Sender-Class:From:To:Subject:Date;
	b=j/q3a/XYQJoXIRsvtlpdUD8UrBKF0W4qBibmH4P6d8rPHr/XpKDGO03SXk79HhA1D
	 sjh7H8CCfOTll+HgkL1JVWiYf/J6e6aRUo9aA8HClaOdSWyqKe7VbezVGXU+dXmPXt
	 xiaTwiHodsqjjftWKr1qgURqyK1w9NImhwypAeNA=
X-UI-Sender-Class: 01bb95c1-4bf8-414a-932a-4f6e2808ef9c
Received: from [91.160.101.96] ([91.160.101.96]) by web-mail.gmx.net
 (3c-app-gmx-bs59.server.lan [172.19.170.143]) (via HTTP); Sat, 16 May 2020
 10:42:59 +0200
MIME-Version: 1.0
Message-ID: <trinity-41fdabeb-5ca0-4635-bf93-2127dec07c30-1589618579392@3c-app-gmx-bs59>
From: "Albrecht Zimmermann" <Albrecht_Zimmermann@gmx.net>
To: "Liste EGC" <polytech_liste-egc@univ-nantes.fr>, liste-proml@lri.fr,
 connectionists@mailman.srv.cs.cmu.edu, siam-dma@siam.org
Content-Type: text/html; charset=UTF-8
Date: Sat, 16 May 2020 10:42:59 +0200
Importance: normal
Sensitivity: Normal
X-Priority: 3
X-Provags-ID: V03:K1:3q0kePozHz5WdyFtL1JfheFTw34w7KZ165Nw8blYgX1Wx3Hmbtt66fi+Jk7OrwWcnZ4P0
 IHIs9pFe5t7LaOHPO672rSVKdrWOwiNbjWyVR1gznTZ7egJN2k2ILuWvH/kFRKNIu2HLSxuEzZHu
 TBv8+S42H+HbkcqUxgcA4L44hdEYxFaM84UWcgnHgItdYloTiW/P7oIoDdD39ksb83HRCG4LIdZB
 jaYuyAP2CKzc8foFoNMxaFrDuKjg4S0EwLZRD0uIm6K4V1rijfAb70s27xLfX7/kJNXdYuuJXAFV
 zY=
X-UI-Out-Filterresults: notjunk:1;V03:K0:cJ2VEQUSoDw=:oINhaYwwv5eISjKBLiFysH
 PFVugeXC6dp3YT74dGf22OWVH8bl7yEgyR4rR2HTVHd20GbP9GDBIPuY5LyAorIug7/bCjSx8
 KNhaSPGpFvLy7XAQcvUksXeWGxUCMR6alzmaj75H8c7B2mwhAOZV6I5BGGXwl1JtKWVI7QMpd
 UbpXzQgvMV33NFqplLv1IH4uCWcisoOl79wAIjqFXx0D+0QrPB544wuNLiuF3P8FGqLL8VGST
 PgkZKUAqYJx4RycFKrh0xHE32DELETzQ67oLrOrZ650WhCbwhlwZlUgujGDHkmejtYJGfL95V
 Fc8QviadCpLGLI9QJ/uTnVjysJQCTobU2+mOzs136cE5LprHR1SEu48EWxCKdeU13W64+Flp1
 usXE7/O609WZF9RgNw321k1TIPykjzKcuNPQersRt4Bee4Pv5oEnxI0Lra1mW1msm3A6FCoWT
 xNGuSxksFYpu8pzAgc8yCUvnNt5yj2afY3eY4AlbC6CO46wZT/6aXC/HTRMGxzDZi/iNww+Pg
 IQPbk0FKxO8LMJbSK2eSShdPOrX5/QCrOOaBgBKSfECcgOHlWTOziVNmVYcD2PM+IrgufupLc
 cOu3YMcGwUQOcjiLCyZtUm2GXYTRU80Ws1ZKwbxqijuBRzrT2/7C0+r2QZr/Yl6iNTcvHm03n
 wmVwayqX+Qr5YK7L5Bg9El3yWgIXY18TOq2nOKN6rXAHyUea6fjBtVyvSTGv/jHBBT924ypdA
 tfnc9/XSvTa+/5Vy
X-Validation-by: p_bruneau@hotmail.com
Subject: [liste-egc] 2nd CfP 2nd Workshop on Evaluation and Experimental
 Design in Data Mining and Machine Learning (EDML 2020) @ ECML PKDD

<html><head></head><body><div style="font-family: Verdana;font-size: 12.0px;"><div>
<div>*Description*</div>

<div>&nbsp;</div>

<div>A vital part of proposing new machine learning and data mining approaches is evaluating them empirically to allow an assessment of their capabilities. Numerous choices go into setting up such evaluations: how to choose the data, how to preprocess them (or not), potential problems associated with the selection of datasets, what other techniques to compare to (if any), what metrics to evaluate, etc. and last but not least how to present and interpret the results.<br/>
Typically, one learns how to make those choices on-the-job, often by copying the evaluation protocols used in the existing literature - a procedure that can easily lead to the development of problematic habits. Numerous, albeit scattered, publications have called attention to those questions [1-5] and have occasionally called into question published results, or the usability of published methods.</div>

<div>&nbsp;</div>

<div>Those studies consider different evaluation aspects in isolation, and the issue becomes even more complex because setting up an experiment introduces additional dependencies and biases: having chosen an evaluation metric with little bias can be easily undermined choosing data that cannot appropriately treated by one of the comparison techniques, for instance, and having carefully addressed both aspects is of little worth if the statistical test chosen does not allow to assess significance.<br/>
At a time of intense discussions about a reproducibility crisis in natural, social, and life sciences, and conferences such as SIGMOD, KDD, and ECML/PKDD encouraging researchers to make their work as reproducible as possible, we therefore feel that it is important to bring researchers together, and discuss those issues on a fundamental level. In non-computational sciences, experimental design has been studied in depth, which has given rise to such principles as randomization, blocking, or factorial experiments. While these principles are usually not applied in machine learning and data mining, one desirable goal that arose during workshop discussions is that of the formulation of a checklist that quickly allows to evaluate the experiment one is about to perform, and to identify and correct weaknesses. An important starting point of any such list has to be: &ldquo;What question do we want to answer?&rdquo;</div>

<div>&nbsp;</div>

<div>An issue directly related to the dataset choice mentioned above is the following: even the best-designed experiment carries only limited information if the underlying data are lacking. We therefore also want to discuss questions related to the availability of data, whether they are reliable, diverse, and whether they correspond to realistic and/or challenging problem settings. This is of particular importance because our field is at a disadvantage compared to other experimental science: whereas there, data are collected (e.g. in social sciences), or generated (e.g. in physics), we often &ldquo;only&rdquo; use existing data.<br/>
&nbsp;<br/>
Finally, we want to emphasize the responsibility of the researchers to communicate their research as objectively as possible. We also want to highlight the critical role of the reviewers: The typical expectation of many reviewers seems to be that an evaluation should demonstrate that a newly proposed method is better than existing work. This can be shown on a few example datasets at most and is still not necessarily true in general. Rather it should be demonstrated in papers (and appreciated by reviewers) to show on what kind of data a new method works well, and also where it does not, and this way in which respect is different from existing work and therefore is a useful complement. A related topic is therefore also how to characterize datasets, e.g., in terms of their learning complexity [6] and how to create benchmark datasets, an essential tool for method development and assessment, adopted by other domains like computer vision, IR etc.</div>

<div>&nbsp;</div>

<div>*Topics*</div>

<div>&nbsp;</div>

<div>In this workshop, we mainly solicit contributions that discuss those questions on a fundamental level, take stock of the state-of-the-art, offer theoretical arguments, or take well-argued positions, as well as actual evaluation papers that offer new insights, e.g. question published results, or shine the spotlight on the characteristics of existing benchmark data sets.</div>

<div>&nbsp;</div>

<div>As such, topics include, but are not limited to:<br/>
- Benchmark datasets for data mining tasks: are they diverse/realistic/challenging?<br/>
- Impact of data quality (redundancy, errors, noise, bias, imbalance, ...) on qualitative evaluation<br/>
- Propagation/amplification of data quality issues on the data mining results (also interplay between data and algorithms)<br/>
- Evaluation of unsupervised data mining (dilemma between novelty and validity)<br/>
- Evaluation measures<br/>
- (Automatic) data quality evaluation tools: What are the aspects one should check before starting to apply algorithms to given data?<br/>
- Issues around runtime evaluation (algorithm vs. implementation, dependency on hardware, algorithm parameters, dataset characteristics)<br/>
- Design guidelines for crowd-sourced evaluations<br/>
- Principled experimental workflows</div>

<div>&nbsp;</div>

<div>The workshop will feature a mix of invited speakers, a number of accepted presentations with ample time for questions since those contributions will be less technical, and more philosophical in nature, and a panel discussion on the current state, and the areas that most urgently need improvement, as well as recommendation to achieve those improvements. Workshop submissions will be published in the CEUR-WS workshop series. An important objective of this workshop is a document synthesizing these discussions that we intend to publish at a more prominent venue.</div>

<div>&nbsp;</div>

<div>*Submission*</div>

<div>&nbsp;</div>

<div>Papers should be submitted as PDF, using the Springer LNCS style, available at https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines. Submissions should be limited to ten pages and submitted via Easychair at https://easychair.org/conferences/?conf=edml20.</div>

<div>Papers will be reviewed by at least two members of the Program Committee on the basis of technical quality, relevance, significance, and clarity. Submitting a paper to the workshop means that if the paper is accepted at least one author should present the paper at the workshop. Accepted papers will be published after the workshop with&nbsp; CEUR-WS or Springer.</div>

<div>&nbsp;</div>

<div>*Important dates*</div>

<div>&nbsp;</div>

<div>Submission deadline: June 09, 2020<br/>
Notification deadline: July 07, 2020<br/>
SDM early bird registration deadline: July 20, 2020<br/>
Camera ready: July 21, 2020<br/>
Conference dates: September 14-18, 2020<br/>
&nbsp;&nbsp; &nbsp;<br/>
*Organizers*</div>

<div>&nbsp;</div>

<div>Eirini Ntoutsi, Leibniz University Hannover &amp; L3S Research Center, Germany, ntoutsi@kbs.uni-hannover.de<br/>
Erich Schubert, Technical University Dortmund, Germany, erich.schubert@cs.tu-dortmund.de<br/>
Arthur Zimek, University of Southern Denmark, zimek@imada.sdu.dk<br/>
Albrecht Zimmermann, University Caen Normandy, France, albrecht.zimmermann@unicaen.fr</div>

<div>&nbsp;</div>

<div>The workshop&#39;s website can be found at https://imada.sdu.dk/Research/EDML/2020/</div>
</div></div></body></html>
